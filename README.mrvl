
Below are instructions for reproducing a working VPP environment on top of Marvell's Armada-7/8K SoCs
The document below specifies the exact versions used for reproducing the system, however, different
versions of these software components might be used, although, this might require some slight modifications
to the installation process.

1. Hardware
-----------
 - Armada-7/8K development / community board.
   (For the purpose of this readme, a 8040-McBin board was used).
 - Traffic generator (connected through the 10G ports).
 - External network access (for github, packages installation...) - through
   port 2 of McBin board (eth2).

2. Basic board preps
---------------------
 - Boot the board using some temporary Kernel / DTB images (temporary, as it
   will be replaced with a new Kernel in section #3).
 - Use ubuntu-arm64 file-system (ubuntu-16.04 was used in our case).
 - The following packages will be required during the installation process:
   # apt-get install git openssh-server make bc
   # apt-get install autoconf automake libtool
   # apt-get install ccache
   # (optional) apt-get install xterm socat zip telnet

3. MUSDK (part 1)
-----------------
 - Clone MUSDK code from github
   # cd /root/
   # git clone https://github.com/MarvellEmbeddedProcessors/musdk-marvell.git -b musdk-armada-17.10 musdk-marvell

4. Kernel
---------
 - Clone Kernel code from github
   # cd /root/
   # git clone https://github.com/MarvellEmbeddedProcessors/linux-marvell.git -b linux-4.4.52-armada-17.10 linux-marvell
 - Apply musdk kernel patches:
   # cd linux-marvell
   # git am /root/musdk-marvell/patches/linux/*.patch
 - Configure and build the Kernel
   # make mvebu_v8_lsp_defconfig
   # make -j4 Image dtbs modules
   # make modules_install
   # make install
   # cp arch/arm64/boot/Image arch/arm64/boot/dts/marvell/armada-8040-mcbin.dtb /boot/
 -  Reboot the board using the Kernel and DTB compiled above.

5. MUSDK (part 2)
-----------------
 - Configure & build musdk libraries / applications
   # cd /root/musdk-marvell
   # export KDIR=/root/linux-marvell
   # ./bootstrap
   # ./configure --enable-shared --enable-bpool-dma=64 --enable-bpool-cookie=32 --enable-sam --prefix=/usr
   # make -j4
   # make install
 - Build & install MUSDK modules
   # cd modules/
   # for i in dmax2 neta pp2 sam uio; do cd $i; make; make -C $KDIR M=`pwd` modules_install; cd -; done

6. pp2-sysfs driver
-------------------
 - Clone mvpp2-sysfs source code from github
   # cd /root/
   # git clone https://github.com/MarvellEmbeddedProcessors/mvpp2x-marvell.git -b mvpp2x-armada-17.10 pp2_sysfs
 - Build and install
   # cd /root/pp2_sysfs/sysfs/
   # cp Makefile_sysfs Makefile
   # export KDIR=/root/linux-marvell
   # export KERNEL_SOURCES=1
   # make
   # make -C $KDIR M=`pwd` modules_install

7. DPDK (Extract Marvell patches only)
--------------------------------------
 - The actual DPDK sources will be downloaded and built as part of VPP.
   We are cloning Marvell's DPDK port, in order to extract required patches for adapting the mainline DPDK
   to marvell network interfaces.
 - Clone Marvell's DPDK code
   # cd /root/
   # git clone https://github.com/MarvellEmbeddedProcessors/dpdk-marvell.git -b dpdk-17.05-armada-17.10 dpdk-marvell
 - Extract Marvell's patches on top of mainline DPDK
   # mkdir /root/dpdk-patches/
   # cd dpdk-marvell
   # git format-patch 2225554..HEAD -o /root/dpdk-patches/

8. VPP
------
 - Clone Marvell's VPP port
   # cd /root/
   # git clone https://github.com/MarvellEmbeddedProcessors/vpp-marvell.git -b vpp-devel vpp-marvell
 - Prepare build env
   # cd vpp-marvell/
   # export DPDK_VERSION=17.05
   # export AESNI=n
   # export LIBMUSDK_PATH=/usr/
   # make install-dep
   # make bootstrap
   # groupadd vpp
 - Copy DPDK patches:
   # cp /root/dpdk-patches/* dpdk/dpdk-17.05_patches/
 - Build vpp:
   # make -j4 build-release
 - Create VPP configuration file
   # mkdir -p /etc/vpp/
   # cp src/scripts/mrvl/mrvl_demo_startup.conf /etc/vpp/startup.conf

Running simple bridging setup (between eth0 & eth1 on McBin board)
-----------------------------
 - Load kernel modules
   # modprobe -a musdk_uio mv_dmax2_uio mv_pp_uio mv_sam_uio mvpp2x_sysfs
- Hugepages configuration
   # echo 1000 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
 - Run VPP
   # cd /root/vpp-marvell/
   # export STARTUP_CONF=/etc/vpp/startup.conf
   # make run-release &
   # ifconfig eth0 up
   # ifconfig eth1 up
 - Connect to VPP CLI:
   # telnet 127.0.0.1 5002
 - (from CLI) create bridge, and associate 2 ports:
   # set interface state TenGigabitEthernet0 up
   # set interface state TenGigabitEthernet1 up
   # create bridge-domain 1
   # set interface l2 bridge TenGigabitEthernet0 1
   # set interface l2 bridge TenGigabitEthernet1 1



Running bridging inside VM (eth0->vm(eth0)->vm(eth1)->eth1)
-----------------------------
 - Load kernel modules
   # modprobe -a musdk_uio mv_dmax2_uio mv_pp_uio mv_sam_uio mvpp2x_sysfs
 - Hugepages configuration
   # echo 1000 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
 - Run VPP
   # cd /root/vpp-marvell/
   # export STARTUP_CONF=/etc/vpp/startup.conf
   # make run-release &
   # ifconfig eth0 up
   # ifconfig eth1 up
 - Connect to VPP CLI:
   # telnet 127.0.0.1 5002
 - (from CLI) create vhost ports:
   # create vhost socket /tmp/sock1.sock server feature-mask 0xFF
   # create vhost socket /tmp/sock2.sock server feature-mask 0xFF
 - (from CLI) link up all ports:
   # set interface state TenGigabitEthernet0 up
   # set interface state TenGigabitEthernet1 up
   # set interface state VirtualEthernet0/0/0 up
   # set interface state VirtualEthernet0/0/1 up
 - setup the bridge domains:
   # set interface l2 bridge TenGigabitEthernet0  1
   # set interface l2 bridge VirtualEthernet0/0/0 1
   # set interface l2 bridge TenGigabitEthernet1  2
   # set interface l2 bridge VirtualEthernet0/0/1 2

 VPP is configured. Now we need to boot a VM to bridge between VirtualEthernet0/0/0 and VirtualEthernet0/0/1
 quit VPP using #quit and continue

 - VM environment setup
    - Install QEMU and screen
	# apt-get install qemu-system-aarch64 screen python

    - Prepare VM linux image
       # cd /root/linux-marvell
       # git am /root/ovs-marvell/linux-guest-patches/*
       # ./scripts/config -e VFIO_NOIOMMU
       # ./scripts/config -e VFIO_NOIOMMU
       # make -j6
       # cp arch/arm64/boot/Image /boot/Image-vm

    - Prepare ubuntu-16.04 file system image for the guest VM
       # dd if=/dev/zero of=/root/ubuntu-16.04-vm-fs.img bs=1M count=2000
       # mkfs.ext4 /root/ubuntu-16.04-vm-fs.img
       # mkdir /mnt/loop
       # mount /root/ubuntu-16.04-vm-fs.img /mnt/loop
       # cd /mnt/loop
       # tar -xf /root/ubuntu-16.04-arm64.tar
       # cp /root/dpdk-marvell /mnt/loop/root/.

    - Install packages for VM file system
       # chroot /mnt/loop
       # dhclient eth2
       # apt-get update
       # apt-get install build-essential pciutils

    - Compile DPDK for VM
      - Disable KNI and MRVL PMD in config file
         - open config/common_linuxapp and delete CONFIG_RTE_KNI_KMOD
         - open config/common_base and delete
	    - CONFIG_RTE_LIBRTE_MRVL_PMD
	    - CONFIG_RTE_LIBRTE_MRVL_DEBUG
	    - CONFIG_RTE_MRVL_MUSDK_DMA_MEMSIZE
	    - CONFIG_RTE_LIBRTE_PMD_MRVL_CRYPTO
	    - CONFIG_RTE_LIBRTE_PMD_MRVL_CRYPTO_DEBUG
      - Build DPDK
	 # make config T=arm64-armv8a-linuxapp-gcc
	 # make
	 # make install

   10.5 Boot the VM
      - Start screen application and create a 2nd terminal
	   # screen
           # Ctrl-a c   " open 2nd terminal "
	   # Ctrl-a 1   " switch to 2nd terminal"
      - Boot VM with QEMU
	   # qemu-system-aarch64 -enable-kvm -nographic -kernel /boot/Image-vm -m 768 -M virt,iommu=on -cpu host -smp 2\
	        -drive file=/root/ubuntu-16.04-vm-fs.img,id=fs,if=none,format=raw -device virtio-blk-device,drive=fs \
	        -chardev socket,id=char0,path=/tmp/sock1.sock \
	        -netdev type=vhost-user,id=net0,chardev=char0,vhostforce  \
	        -device virtio-net-pci,mac=00:00:00:00:00:10,netdev=net0 \
	        -chardev socket,id=char1,path=/tmp/sock2.sock \
	        -netdev type=vhost-user,id=net1,chardev=char1,vhostforce \
	        -device virtio-net-pci,mac=00:00:00:00:00:11,netdev=net1 \
	        -object memory-backend-file,share=on,id=mem,size=768M,mem-path=/dev/hugepages \
	        -numa node,memdev=mem -mem-prealloc \
	        -append "earlyprintk console=ttyAMA0 isolcpus=1 rootwait root=/dev/vda rw"

       - Verify Virtio PCI devices exist. lspci should show
		00:00.0 Host bridge: Red Hat, Inc. Device 0008
		00:01.0 Ethernet controller: Red Hat, Inc Virtio network device
		00:02.0 Ethernet controller: Red Hat, Inc Virtio network device

    10.6 Run DPDK on VM
       - system setup
          # init s  /*  to eliminate un-needed processes from ubuntu */
	  # echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
          # mkdir /mnt/huge
          # mount -t hugetlbfs nodev /mnt/huge
          # echo 1 > /sys/module/vfio/parameters/enable_unsafe_noiommu_mode
          # dpdk-devbind -b vfio-pci 0000:00:01.0
          # dpdk-devbind -b vfio-pci 0000:00:02.0

       - dpdk run
          # testpmd -c 0x3 -- -i -a --total-num-mbufs=16384

     The system is now ready to bridge traffic.
     Back on the hypervisor, check that you have 2 threads occupying 100% CPU.
     Make sure none of these threads is running on CPU-0. If one of them does,
     move it to CPUs 1,2, or 3 using taskset.
