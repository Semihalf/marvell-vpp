From 6fad1a09e577f9e9ad3fc055198c4c58a4f8256d Mon Sep 17 00:00:00 2001
From: Zyta Szpak <zr@semihalf.com>
Date: Mon, 19 Mar 2018 17:20:58 +0100
Subject: [PATCH 64/74] net/mvneta: rename mrvlna to mvneta pmd driver

---
 config/common_base                            |    4 +-
 doc/guides/nics/mrvl.rst                      |    4 +-
 drivers/net/Makefile                          |    2 +-
 drivers/net/mrvlna/Makefile                   |   68 --
 drivers/net/mrvlna/mrvlna_ethdev.c            | 1365 -------------------------
 drivers/net/mrvlna/mrvlna_ethdev.h            |  101 --
 drivers/net/mrvlna/rte_pmd_mrvlna_version.map |    3 -
 drivers/net/mvneta/Makefile                   |   69 ++
 drivers/net/mvneta/mvneta_ethdev.c            | 1365 +++++++++++++++++++++++++
 drivers/net/mvneta/mvneta_ethdev.h            |  101 ++
 drivers/net/mvneta/rte_pmd_mvneta_version.map |    3 +
 mk/rte.app.mk                                 |    2 +-
 12 files changed, 1544 insertions(+), 1543 deletions(-)
 delete mode 100644 drivers/net/mrvlna/Makefile
 delete mode 100644 drivers/net/mrvlna/mrvlna_ethdev.c
 delete mode 100644 drivers/net/mrvlna/mrvlna_ethdev.h
 delete mode 100644 drivers/net/mrvlna/rte_pmd_mrvlna_version.map
 create mode 100644 drivers/net/mvneta/Makefile
 create mode 100644 drivers/net/mvneta/mvneta_ethdev.c
 create mode 100644 drivers/net/mvneta/mvneta_ethdev.h
 create mode 100644 drivers/net/mvneta/rte_pmd_mvneta_version.map

diff --git a/config/common_base b/config/common_base
index 52553ed..8b64e00 100644
--- a/config/common_base
+++ b/config/common_base
@@ -385,9 +385,9 @@ CONFIG_RTE_LIBRTE_PMD_FAILSAFE=y
 CONFIG_RTE_LIBRTE_MRVL_PMD=n
 
 #
-# Compile Marvell NETA PMD driver
+# Compile Marvell MVNETA PMD driver
 #
-CONFIG_RTE_LIBRTE_MRVLNA_PMD=n
+CONFIG_RTE_LIBRTE_MVNETA_PMD=n
 
 #
 # Compile virtual device driver for NetVSC on Hyper-V/Azure
diff --git a/doc/guides/nics/mrvl.rst b/doc/guides/nics/mrvl.rst
index 22ecfd5..f40ba6d 100644
--- a/doc/guides/nics/mrvl.rst
+++ b/doc/guides/nics/mrvl.rst
@@ -134,9 +134,9 @@ The following options can be modified in the ``config`` file.
 
     Toggle compilation of the librte_pmd_mrvl driver.
    
-- ``CONFIG_RTE_LIBRTE_MRVLNA_PMD`` (default ``n``)
+- ``CONFIG_RTE_LIBRTE_MVNETA_PMD`` (default ``n``)
 
-    Toggle compilation of the librte_pmd_mrvlna driver.
+    Toggle compilation of the librte_pmd_mvneta driver.
 
 
 QoS Configuration
diff --git a/drivers/net/Makefile b/drivers/net/Makefile
index 1fa8264..798ff48 100644
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -28,7 +28,7 @@ DIRS-$(CONFIG_RTE_LIBRTE_LIO_PMD) += liquidio
 DIRS-$(CONFIG_RTE_LIBRTE_MLX4_PMD) += mlx4
 DIRS-$(CONFIG_RTE_LIBRTE_MLX5_PMD) += mlx5
 DIRS-$(CONFIG_RTE_LIBRTE_MRVL_PMD) += mrvl
-DIRS-$(CONFIG_RTE_LIBRTE_MRVLNA_PMD) += mrvlna
+DIRS-$(CONFIG_RTE_LIBRTE_MVNETA_PMD) += mvneta
 DIRS-$(CONFIG_RTE_LIBRTE_NFP_PMD) += nfp
 DIRS-$(CONFIG_RTE_LIBRTE_BNXT_PMD) += bnxt
 DIRS-$(CONFIG_RTE_LIBRTE_PMD_NULL) += null
diff --git a/drivers/net/mrvlna/Makefile b/drivers/net/mrvlna/Makefile
deleted file mode 100644
index 8980268..0000000
--- a/drivers/net/mrvlna/Makefile
+++ /dev/null
@@ -1,68 +0,0 @@
-#   BSD LICENSE
-#
-#   Copyright(c) 2017 Marvell International Ltd.
-#   Copyright(c) 2017 Semihalf.
-#   All rights reserved.
-#
-#   Redistribution and use in source and binary forms, with or without
-#   modification, are permitted provided that the following conditions
-#   are met:
-#
-#     * Redistributions of source code must retain the above copyright
-#       notice, this list of conditions and the following disclaimer.
-#     * Redistributions in binary form must reproduce the above copyright
-#       notice, this list of conditions and the following disclaimer in
-#       the documentation and/or other materials provided with the
-#       distribution.
-#     * Neither the name of the copyright holder nor the names of its
-#       contributors may be used to endorse or promote products derived
-#       from this software without specific prior written permission.
-#
-#   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
-#   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
-#   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
-#   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
-#   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
-#   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
-#   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
-#   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
-#   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
-#   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-include $(RTE_SDK)/mk/rte.vars.mk
-
-ifneq ($(MAKECMDGOALS),clean)
-ifneq ($(MAKECMDGOALS),config)
-ifeq ($(LIBMUSDK_PATH),)
-$(error "Please define LIBMUSDK_PATH environment variable")
-endif
-endif
-endif
-
-# library name
-LIB = librte_pmd_mrvlna.a
-
-# library version
-LIBABIVER := 1
-
-# versioning export map
-EXPORT_MAP := rte_pmd_mrvlna_version.map
-
-# external library dependencies
-CFLAGS += -I$(LIBMUSDK_PATH)/include
-CFLAGS += -DMVCONF_TYPES_PUBLIC
-CFLAGS += -DMVCONF_DMA_PHYS_ADDR_T_PUBLIC
-CFLAGS += -DMVCONF_PP2_BPOOL_COOKIE_SIZE=32 -DMVCONF_PP2_BPOOL_DMA_ADDR_SIZE=64 -DMVCONF_DMA_PHYS_ADDR_T_SIZE=64
-CFLAGS += $(WERROR_FLAGS)
-CFLAGS += -O3
-LDLIBS += -L$(LIBMUSDK_PATH)/lib
-LDLIBS += -lmusdk
-LDLIBS += -lrte_eal -lrte_mbuf -lrte_mempool -lrte_ring
-LDLIBS += -lrte_ethdev -lrte_net -lrte_kvargs -lrte_cfgfile
-LDLIBS += -lrte_bus_vdev
-
-# library source files
-SRCS-$(CONFIG_RTE_LIBRTE_MRVLNA_PMD) += mrvlna_ethdev.c
-
-include $(RTE_SDK)/mk/rte.lib.mk
diff --git a/drivers/net/mrvlna/mrvlna_ethdev.c b/drivers/net/mrvlna/mrvlna_ethdev.c
deleted file mode 100644
index 10135d0..0000000
--- a/drivers/net/mrvlna/mrvlna_ethdev.c
+++ /dev/null
@@ -1,1365 +0,0 @@
-/*-
- *   BSD LICENSE
- *
- *   Copyright(c) 2017 Marvell International Ltd.
- *   Copyright(c) 2017 Semihalf.
- *   All rights reserved.
- *
- *   Redistribution and use in source and binary forms, with or without
- *   modification, are permitted provided that the following conditions
- *   are met:
- *
- *     * Redistributions of source code must retain the above copyright
- *       notice, this list of conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright
- *       notice, this list of conditions and the following disclaimer in
- *       the documentation and/or other materials provided with the
- *       distribution.
- *     * Neither the name of Semihalf nor the names of its
- *       contributors may be used to endorse or promote products derived
- *       from this software without specific prior written permission.
- *
- *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <rte_ethdev_driver.h>
-#include <rte_kvargs.h>
-#include <rte_log.h>
-#include <rte_malloc.h>
-#include <rte_bus_vdev.h>
-
-#include <stdio.h>
-#include <fcntl.h>
-#include <linux/ethtool.h>
-#include <linux/sockios.h>
-#include <net/if.h>
-#include <net/if_arp.h>
-#include <sys/ioctl.h>
-#include <sys/socket.h>
-#include <sys/stat.h>
-#include <sys/types.h>
-
-#include "mrvlna_ethdev.h"
-
-
-#define MRVL_IFACE_NAME_ARG "iface"
-#define MRVL_CFG_ARG "cfg"
-
-#define MRVLNA_COOKIE_ADDR_INVALID ~0ULL
-
-#define MRVLNA_COOKIE_HIGH_ADDR_SHIFT	(sizeof(neta_cookie_t) * 8)
-#define MRVLNA_COOKIE_HIGH_ADDR_MASK	(~0ULL << MRVLNA_COOKIE_HIGH_ADDR_SHIFT)
-
-#define MRVLNA_MUSDK_DMA_MEMSIZE 41943040 /* (40 * 1024 * 1024) */
-
-#define MRVLNA_PKT_SIZE_MAX (16382 - MV_MH_SIZE) /* 9700B */
-#define MRVLNA_DEFAULT_MTU	1500
-
-#define MRVLNA_MAC_ADDRS_MAX 256 /*16 UC, 256 IP, 256 MC/BC */
-/** Maximum length of a match string */
-#define MRVLNA_MATCH_LEN 16
-
-#define MRVLNA_PKT_EFFEC_OFFS (MRVL_NETA_PKT_OFFS + MV_MH_SIZE)
-
-uint64_t cookie_addr_high = MRVLNA_COOKIE_ADDR_INVALID;
-uint16_t rx_desc_free_thresh = MRVL_NETA_BUF_RELEASE_BURST_SIZE;
-
-static const char * const valid_args[] = {
-	MRVL_IFACE_NAME_ARG,
-	NULL
-};
-
-struct mrvlna_ifnames {
-	const char *names[NETA_NUM_ETH_PPIO];
-	int idx;
-};
-
-/*
- * To use buffer harvesting based on loopback port shadow queue structure
- * was introduced for buffers information bookkeeping.
- *
- * Before sending the packet, related buffer information is
- * stored in shadow queue. After packet is transmitted no longer used
- * packet buffer is released back to it's original hardware pool,
- * on condition it originated from interface.
- * In case it  was generated by application itself i.e: mbuf->port field is
- * 0xff then its released to software mempool.
- */
-struct neta_shadow_txq {
-	int head;           /* write index - used when sending buffers */
-	int tail;           /* read index - used when releasing buffers */
-	u16 size;           /* queue occupied size */
-	u16 num_to_release; /* number of buffers sent, that can be released */
-	struct neta_buff_inf ent[MRVL_NETA_TX_SHADOWQ_SIZE]; /* q entries */
-};
-
-
-struct neta_rxq {
-	struct neta_priv *priv;
-	struct rte_mempool *mp;
-	int queue_id;
-	int port_id;
-	int size;
-	int cksum_enabled;
-	uint64_t bytes_recv;
-	uint64_t drop_mac;
-	uint64_t pkts_processed;
-};
-
-
-struct neta_txq {
-	struct neta_priv *priv;
-	int queue_id;
-	int port_id;
-	uint64_t bytes_sent;
-	struct neta_shadow_txq shadow_txqs[RTE_MAX_LCORE];
-	int tx_deferred_start;
-};
-
-static int mrvlna_dev_num;
-static int mrvlna_lcore_first;
-static int mrvlna_lcore_last;
-
-
-
-/**
- * Allocate buffers from mempool
- * and store addresses in rx descriptors.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_buffs_alloc(struct neta_priv *priv, struct neta_rxq *rxq, int num)
-{
-	struct rte_mbuf *mbufs[MRVL_NETA_TXD_MAX];
-	struct neta_buff_inf entries[MRVL_NETA_TXD_MAX];
-	uint16_t nb_desc;
-	int ret, i;
-
-	nb_desc = num;
-	ret = rte_pktmbuf_alloc_bulk(rxq->mp, mbufs, nb_desc);
-	if (ret)
-		RTE_LOG(ERR, PMD,
-				"Failed to allocate %u mbufs.\n", nb_desc);
-
-
-	if (cookie_addr_high == MRVLNA_COOKIE_ADDR_INVALID)
-		cookie_addr_high =
-			(uint64_t)mbufs[0] & MRVLNA_COOKIE_HIGH_ADDR_MASK;
-
-	for (i = 0; i < nb_desc; i++) {
-		if (((uint64_t)mbufs[i] & MRVLNA_COOKIE_HIGH_ADDR_MASK)
-			!= cookie_addr_high) {
-			RTE_LOG(ERR, PMD,
-				"mbuf virtual addr high 0x%lx out of range\n",
-				(uint64_t)mbufs[i] >> 32);
-			goto out;
-		}
-	}
-
-	for (i = 0; i < nb_desc; i++) {
-		entries[i].addr = rte_mbuf_data_iova_default(mbufs[i]);
-		entries[i].cookie = (neta_cookie_t)(uint64_t)mbufs[i];
-	}
-	ret = neta_ppio_inq_put_buffs(priv->ppio, rxq->queue_id, entries, &nb_desc);
-	if (ret) {
-		RTE_LOG(ERR, PMD,
-				"Failed to fill rx desc\n");
-		return ret;
-	}
-
-	return 0;
-
-out:
-	for (; i < nb_desc; i++)
-		rte_pktmbuf_free(mbufs[i]);
-
-	return -1;
-}
-
-/**
- * Return mbufs to mempool.
- *
- * @param rxq
- *    Pointer to rx queue structure
- * @param desc
- *    Array of rx descriptors
- */
-static void
-mrvlna_recv_buffs_free(struct neta_ppio_desc *desc, uint16_t num)
-{
-	uint64_t addr;
-	uint8_t i;
-
-	for (i = 0; i < num; i++) {
-		if (desc) {
-			addr = cookie_addr_high |
-					neta_ppio_inq_desc_get_cookie(desc);
-			if (addr)
-				rte_pktmbuf_free((struct rte_mbuf *)addr);
-			desc++;
-		}
-	}
-}
-
-/**
- * Release already sent buffers to mempool.
- *
- * @param ppio
- *   Pointer to the port structure.
- * @param sq
- *   Pointer to the shadow queue.
- * @param qid
- *   Queue id number.
- * @param force
- *   Force releasing packets.
- */
-static inline void
-mrvlna_sent_buffers_free(struct neta_ppio *ppio,
-		struct neta_shadow_txq *sq, int qid, int force)
-{
-	struct neta_buff_inf *entry;
-	uint16_t nb_done = 0;
-	int i;
-	int tail = sq->tail;
-
-	neta_ppio_get_num_outq_done(ppio, qid, &nb_done);
-
-	sq->num_to_release += nb_done;
-
-	if (likely(!force &&
-		   sq->num_to_release < MRVL_NETA_BUF_RELEASE_BURST_SIZE))
-		return;
-
-	nb_done = sq->num_to_release;
-	sq->num_to_release = 0;
-
-	for (i = 0; i < nb_done; i++) {
-		entry = &sq->ent[tail];
-
-		if (unlikely(!entry->addr)) {
-			RTE_LOG(ERR, PMD,
-				"Shadow memory @%d: cookie(%lx), pa(%lx)!\n",
-				sq->tail, (u64)entry->cookie,
-				(u64)entry->addr);
-			tail = (tail + 1) & MRVL_NETA_TX_SHADOWQ_MASK;;
-			continue;
-		}
-
-		struct rte_mbuf *mbuf;
-
-		mbuf = (struct rte_mbuf *)
-			   (cookie_addr_high | entry->cookie);
-		rte_pktmbuf_free(mbuf);
-		tail = (tail + 1) & MRVL_NETA_TX_SHADOWQ_MASK;
-	}
-
-	sq->tail = tail;
-	sq->size -= nb_done;
-}
-
-/**
- * Flush single receive queue.
- *
- * @param rxq
- *   Pointer to rx queue structure.
- * @param descs
- *   Array of rx descriptors
- */
-static void
-mrvlna_rx_queue_flush(struct neta_rxq *rxq, struct neta_ppio_desc *descs)
-{
-	int ret, num;
-
-	do {
-		num = MRVL_NETA_RXD_MAX;
-		ret = neta_ppio_recv(rxq->priv->ppio,
-					rxq->queue_id,
-					descs, (uint16_t *)&num);
-		mrvlna_recv_buffs_free(descs, num);
-		rxq->pkts_processed += num;
-	} while (ret == 0 && num);
-
-}
-
-/**
- * Flush single transmit queue.
- *
- * @param txq
- *     Pointer to tx queue structure
- */
-static void
-mrvlna_tx_queue_flush(struct neta_txq *txq)
-{
-	int i;
-
-	for (i = 0; i < RTE_MAX_LCORE; i++) {
-		struct neta_shadow_txq *sq =
-				&txq->shadow_txqs[i];
-
-		/* free the rest of them */
-		while (sq->tail != sq->head) {
-			uint64_t addr = cookie_addr_high |
-				sq->ent[sq->tail].cookie;
-			rte_pktmbuf_free(
-				(struct rte_mbuf *)addr);
-			sq->tail = (sq->tail + 1) & MRVL_NETA_TX_SHADOWQ_MASK;
-		}
-		memset(sq, 0, sizeof(*sq));
-	}
-}
-
-/**
- * Deinitialize packet processor.
- */
-static void
-mrvlna_neta_deinit(void)
-{
-	neta_deinit();
-}
-
-/**
- * Initialize packet processor.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_neta_init(void)
-{
-	return neta_init();
-}
-
-/**
- * Callback used by rte_kvargs_process() during argument parsing.
- *
- * @param key
- *   Pointer to the parsed key (unused).
- * @param value
- *   Pointer to the parsed value.
- * @param extra_args
- *   Pointer to the extra arguments which contains address of the
- *   table of pointers to parsed interface names.
- *
- * @return
- *   Always 0.
- */
-static int
-mrvlna_ifnames_get(const char *key __rte_unused, const char *value,
-		 void *extra_args)
-{
-	struct mrvlna_ifnames *ifnames = extra_args;
-
-	ifnames->names[ifnames->idx++] = value;
-
-	return 0;
-}
-
-/**
- * Return packet type information and l3/l4 offsets.
- *
- * @param desc
- *   Pointer to the received packet descriptor.
- * @param l3_offset
- *   l3 packet offset.
- * @param l4_offset
- *   l4 packet offset.
- *
- * @return
- *   Packet type information.
- */
-static inline uint64_t
-mrvlna_desc_to_packet_type_and_offset(struct neta_ppio_desc *desc,
-				    uint8_t *l3_offset, uint8_t *l4_offset)
-{
-	enum neta_inq_l3_type l3_type;
-	enum neta_inq_l4_type l4_type;
-	uint64_t packet_type;
-
-	neta_ppio_inq_desc_get_l3_info(desc, &l3_type, l3_offset);
-	neta_ppio_inq_desc_get_l4_info(desc, &l4_type, l4_offset);
-
-	packet_type = RTE_PTYPE_L2_ETHER;
-
-	switch (l3_type) {
-	case NETA_INQ_L3_TYPE_IPV4_BAD:
-	case NETA_INQ_L3_TYPE_IPV4_OK:
-		packet_type |= RTE_PTYPE_L3_IPV4;
-		break;
-	case NETA_INQ_L3_TYPE_IPV6:
-		packet_type |= RTE_PTYPE_L3_IPV6;
-		break;
-	default:
-		packet_type |= RTE_PTYPE_UNKNOWN;
-		RTE_LOG(DEBUG, PMD, "Failed to recognize l3 packet type\n");
-		break;
-	}
-
-	switch (l4_type) {
-	case NETA_INQ_L4_TYPE_TCP:
-		packet_type |= RTE_PTYPE_L4_TCP;
-		break;
-	case NETA_INQ_L4_TYPE_UDP:
-		packet_type |= RTE_PTYPE_L4_UDP;
-		break;
-	default:
-		packet_type |= RTE_PTYPE_UNKNOWN;
-		RTE_LOG(DEBUG, PMD, "Failed to recognize l4 packet type\n");
-		break;
-	}
-
-	return packet_type;
-}
-
-/**
- * DPDK callback for transmit.
- *
- * @param txq
- *   Generic pointer transmit queue.
- * @param tx_pkts
- *   Packets to transmit.
- * @param nb_pkts
- *   Number of packets in array.
- *
- * @return
- *   Number of packets successfully transmitted.
- */
-static uint16_t
-mrvlna_tx_pkt_burst(void *txq, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
-{
-	struct neta_txq *q = txq;
-	struct neta_shadow_txq *sq;
-	struct neta_ppio_desc descs[nb_pkts];
-	unsigned int core_id = rte_lcore_id();
-
-	int i, bytes_sent = 0;
-	uint16_t num, sq_free_size;
-	uint64_t addr;
-
-	sq = &q->shadow_txqs[core_id];
-	if (unlikely(!q->priv->ppio))
-		return 0;
-
-	if (sq->size)
-		mrvlna_sent_buffers_free(q->priv->ppio,
-				sq, q->queue_id, 0);
-
-	sq_free_size = MRVL_NETA_TX_SHADOWQ_SIZE - sq->size - 1;
-	if (unlikely(nb_pkts > sq_free_size)) {
-		RTE_LOG(DEBUG, PMD,
-			"No room in shadow queue for %d packets! %d packets will be sent.\n",
-			nb_pkts, sq_free_size);
-		nb_pkts = sq_free_size;
-	}
-
-
-	for (i = 0; i < nb_pkts; i++) {
-		struct rte_mbuf *mbuf = tx_pkts[i];
-
-		sq->ent[sq->head].cookie = (neta_cookie_t)(uint64_t)mbuf;
-		sq->ent[sq->head].addr = rte_mbuf_data_iova_default(mbuf);
-		sq->head = (sq->head + 1) & MRVL_NETA_TX_SHADOWQ_MASK;
-		sq->size++;
-
-		neta_ppio_outq_desc_reset(&descs[i]);
-		neta_ppio_outq_desc_set_phys_addr(&descs[i],
-						 rte_pktmbuf_iova(mbuf));
-		neta_ppio_outq_desc_set_pkt_offset(&descs[i], 0);
-		neta_ppio_outq_desc_set_pkt_len(&descs[i],
-					       (rte_pktmbuf_pkt_len(mbuf) - (ETH_FCS_LEN + MV_MH_SIZE)));
-
-		bytes_sent += rte_pktmbuf_pkt_len(mbuf);
-
-		/* TODO : prepare & set proto info in the descriptor */
-
-	}
-	num = nb_pkts;
-	neta_ppio_send(q->priv->ppio, q->queue_id, descs, &nb_pkts);
-
-
-	/* number of packets that were not sent */
-	if (unlikely(num > nb_pkts)) {
-		for (i = nb_pkts; i < num; i++) {
-			sq->head = (MRVL_NETA_TX_SHADOWQ_SIZE + sq->head - 1) &
-				MRVL_NETA_TX_SHADOWQ_MASK;
-			addr = cookie_addr_high | sq->ent[sq->head].cookie;
-			bytes_sent -=
-				rte_pktmbuf_pkt_len((struct rte_mbuf *)addr);
-		}
-		sq->size -= num - nb_pkts;
-	}
-
-	q->bytes_sent += bytes_sent;
-
-	return nb_pkts;
-}
-
-/**
- * DPDK callback for receive.
- *
- * @param rxq
- *   Generic pointer to the receive queue.
- * @param rx_pkts
- *   Array to store received packets.
- * @param nb_pkts
- *   Maximum number of packets in array.
- *
- * @return
- *   Number of packets successfully received.
- */
-static uint16_t
-mrvlna_rx_pkt_burst(void *rxq, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
-{
-	struct neta_rxq *q = rxq;
-	struct neta_ppio_desc descs[nb_pkts];
-	int i, ret, rx_done = 0;
-
-	if (unlikely(!q->priv->ppio))
-		return 0;
-
-	ret = neta_ppio_recv(q->priv->ppio, q->queue_id,
-			descs, &nb_pkts);
-
-	if (unlikely(ret < 0)) {
-		RTE_LOG(ERR, PMD, "Failed to receive packets\n");
-		return 0;
-	}
-
-	for (i = 0; i < nb_pkts; i++) {
-		struct rte_mbuf *mbuf;
-		uint8_t l3_offset, l4_offset;
-		enum neta_inq_desc_status status;
-		uint64_t addr;
-
-		addr = cookie_addr_high |
-			neta_ppio_inq_desc_get_cookie(&descs[i]);
-		mbuf = (struct rte_mbuf *)addr;
-
-		rte_pktmbuf_reset(mbuf);
-
-		/* drop packet in case of mac, overrun or resource error */
-		status = neta_ppio_inq_desc_get_l2_pkt_error(&descs[i]);
-		if (unlikely(status != NETA_DESC_ERR_OK)) {
-			/* Release the mbuf to the mempool since
-			 * it won't be transferred to tx path */
-			rte_pktmbuf_free(mbuf);
-			q->drop_mac++;
-			continue;
-		}
-
-		mbuf->data_off += MRVLNA_PKT_EFFEC_OFFS;
-		mbuf->pkt_len = neta_ppio_inq_desc_get_pkt_len(&descs[i]);
-		mbuf->data_len = mbuf->pkt_len;
-		mbuf->port = q->port_id;
-		mbuf->packet_type =
-			mrvlna_desc_to_packet_type_and_offset(&descs[i],
-								&l3_offset,
-								&l4_offset);
-		mbuf->l2_len = l3_offset;
-		mbuf->l3_len = l4_offset - l3_offset;
-
-		/* TODO set here offloads&recv checksum */
-
-		rx_pkts[rx_done++] = mbuf;
-		q->bytes_recv += mbuf->pkt_len;
-	}
-	q->pkts_processed += rx_done;
-
-	if (q->pkts_processed > rx_desc_free_thresh) {
-		ret = mrvlna_buffs_alloc(q->priv, q, rx_desc_free_thresh);
-		if (ret)
-			RTE_LOG(ERR, PMD, "Refill failed\n");
-		q->pkts_processed -= rx_desc_free_thresh;
-	}
-
-	return rx_done;
-}
-
-/**
- * Ethernet device configuration.
- *
- * Prepare the driver for a given number of TX and RX queues and
- * configure RSS if supported.
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_dev_configure(struct rte_eth_dev *dev)
-{
-	struct neta_priv *priv = dev->data->dev_private;
-
-	if (dev->data->dev_conf.rxmode.mq_mode != ETH_MQ_RX_NONE) {
-		RTE_LOG(INFO, PMD, "Unsupported RSS and rx multi queue mode %d\n",
-			dev->data->dev_conf.rxmode.mq_mode);
-		return -EINVAL;
-	}
-
-	if (!dev->data->dev_conf.rxmode.hw_strip_crc) {
-		RTE_LOG(INFO, PMD,
-			"L2 CRC stripping is always enabled in hw\n");
-		dev->data->dev_conf.rxmode.hw_strip_crc = 1;
-	}
-
-	if (dev->data->dev_conf.rxmode.hw_vlan_strip) {
-		RTE_LOG(INFO, PMD, "VLAN stripping not supported\n");
-		return -EINVAL;
-	}
-
-	if (dev->data->dev_conf.rxmode.split_hdr_size) {
-		RTE_LOG(INFO, PMD, "Split headers not supported\n");
-		return -EINVAL;
-	}
-
-	if (dev->data->dev_conf.rxmode.enable_scatter) {
-		RTE_LOG(INFO, PMD, "RX Scatter/Gather not supported\n");
-		return -EINVAL;
-	}
-
-	if (dev->data->dev_conf.rxmode.enable_lro) {
-		RTE_LOG(INFO, PMD, "LRO not supported\n");
-		return -EINVAL;
-	}
-
-	if (dev->data->dev_conf.rxmode.jumbo_frame)
-		dev->data->mtu = dev->data->dev_conf.rxmode.max_rx_pkt_len -
-				 ETHER_HDR_LEN - ETHER_CRC_LEN;
-
-	priv->ppio_params.outqs_params.num_outqs = dev->data->nb_tx_queues;
-	priv->nb_rx_queues = dev->data->nb_rx_queues;
-	/* Default: 1 TC, no QoS supported. */
-	priv->ppio_params.inqs_params.num_tcs = 1;
-	priv->ppio_params.inqs_params.tcs_params[0].pkt_offset = MRVL_NETA_PKT_OFFS;
-	/* TODO check if DPDK has already set mtu to default value */
-	priv->ppio_params.inqs_params.mtu = dev->data->mtu ? dev->data->mtu : MRVLNA_DEFAULT_MTU;
-
-	return 0;
-}
-
-/**
- * DPDK callback to get information about the device.
- *
- * @param dev
- *   Pointer to Ethernet device structure (unused).
- * @param info
- *   Info structure output buffer.
- */
-static void
-mrvlna_dev_infos_get(struct rte_eth_dev *dev __rte_unused,
-		   struct rte_eth_dev_info *info)
-{
-	info->speed_capa = ETH_LINK_SPEED_10M |
-			   ETH_LINK_SPEED_100M |
-			   ETH_LINK_SPEED_1G |
-			   ETH_LINK_SPEED_2_5G;
-
-	info->max_rx_queues = MRVL_NETA_RXQ_MAX;
-	info->max_tx_queues = MRVL_NETA_TXQ_MAX;
-	info->max_mac_addrs = MRVLNA_MAC_ADDRS_MAX;
-
-	info->rx_desc_lim.nb_max = MRVL_NETA_RXD_MAX;
-	info->rx_desc_lim.nb_min = MRVL_NETA_RXD_MIN;
-	info->rx_desc_lim.nb_align = MRVL_NETA_RXD_ALIGN;
-
-	info->tx_desc_lim.nb_max = MRVL_NETA_TXD_MAX;
-	info->tx_desc_lim.nb_min = MRVL_NETA_TXD_MIN;
-	info->tx_desc_lim.nb_align = MRVL_NETA_TXD_ALIGN;
-
-	info->rx_offload_capa = DEV_RX_OFFLOAD_JUMBO_FRAME |
-				DEV_RX_OFFLOAD_IPV4_CKSUM |
-				DEV_RX_OFFLOAD_UDP_CKSUM |
-				DEV_RX_OFFLOAD_TCP_CKSUM;
-
-	info->tx_offload_capa = DEV_TX_OFFLOAD_IPV4_CKSUM |
-				DEV_TX_OFFLOAD_UDP_CKSUM |
-				DEV_TX_OFFLOAD_TCP_CKSUM;
-
-	/* By default packets are dropped if no descriptors are available */
-	info->default_rxconf.rx_drop_en = 1;
-
-	info->max_rx_pktlen = MRVLNA_PKT_SIZE_MAX;
-}
-
-/**
- * DPDK callback to change the MTU.
- *
- * Setting the MTU affects hardware MRU (packets larger than the MRU
- * will be dropped).
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- * @param mtu
- *   New MTU.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_mtu_set(struct rte_eth_dev *dev, uint16_t mtu)
-{
-	struct neta_priv *priv = dev->data->dev_private;
-	uint16_t mru = MRVL_NETA_MTU_TO_MRU(mtu);
-
-	if (mtu < ETHER_MIN_MTU || mru > MRVLNA_PKT_SIZE_MAX) {
-		RTE_LOG(ERR, PMD, "Invalid MTU [%u] or MRU [%u]\n", mtu, mru);
-		return -EINVAL;
-	}
-
-	if (!priv->ppio)
-		return -EPERM;
-
-	/* TODO note this has no effect as mtu set only during initialization */
-	priv->ppio_params.inqs_params.mtu = mtu;
-
-	/* TODO below functions cause hardware undefined behaviour, skipped for now */
-	/*ret = neta_ppio_set_mru(priv->ppio, mru);
-	if (ret)
-		return ret;
-
-	return neta_ppio_set_mtu(priv->ppio, mtu);*/
-	return 0;
-}
-
-/**
- * DPDK callback to bring the link up.
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_dev_set_link_up(struct rte_eth_dev *dev)
-{
-	struct neta_priv *priv = dev->data->dev_private;
-	int ret;
-
-	if (!priv->ppio)
-		return -EPERM;
-
-	ret = neta_ppio_enable(priv->ppio);
-	if (ret)
-		return ret;
-
-	ret = mrvlna_mtu_set(dev, dev->data->mtu);
-	if (ret) {
-		neta_ppio_disable(priv->ppio);
-		return ret;
-	}
-
-	return 0;
-}
-
-/**
- * DPDK callback to bring the link down.
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_dev_set_link_down(struct rte_eth_dev *dev)
-{
-	struct neta_priv *priv = dev->data->dev_private;
-
-	if (!priv->ppio)
-		return -EPERM;
-
-	return neta_ppio_disable(priv->ppio);
-}
-
-/**
- * DPDK callback to configure the receive queue.
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- * @param idx
- *   RX queue index.
- * @param desc
- *   Number of descriptors to configure in queue.
- * @param socket
- *   NUMA socket on which memory must be allocated.
- * @param conf
- *   Thresholds parameters (unused_).
- * @param mp
- *   Memory pool for buffer allocations.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_rx_queue_setup(struct rte_eth_dev *dev, uint16_t idx, uint16_t desc,
-		    unsigned int socket,
-		    const struct rte_eth_rxconf *conf __rte_unused,
-		    struct rte_mempool *mp)
-{
-	struct neta_priv *priv = dev->data->dev_private;
-	struct neta_rxq *rxq;
-	uint32_t min_size,
-		 max_rx_pkt_len = dev->data->dev_conf.rxmode.max_rx_pkt_len;
-
-	min_size = rte_pktmbuf_data_room_size(mp) - RTE_PKTMBUF_HEADROOM -
-		   MRVLNA_PKT_EFFEC_OFFS;
-	if (min_size < max_rx_pkt_len) {
-		RTE_LOG(ERR, PMD,
-			"Mbuf size must be increased to %u bytes to hold up to %u bytes of data.\n",
-			max_rx_pkt_len + RTE_PKTMBUF_HEADROOM +
-			MRVLNA_PKT_EFFEC_OFFS,
-			max_rx_pkt_len);
-		return -EINVAL;
-	}
-
-	if (dev->data->rx_queues[idx]) {
-		rte_free(dev->data->rx_queues[idx]);
-		dev->data->rx_queues[idx] = NULL;
-	}
-
-	rxq = rte_zmalloc_socket("rxq", sizeof(*rxq), 0, socket);
-	if (!rxq)
-		return -ENOMEM;
-
-	rxq->priv = priv;
-	rxq->mp = mp;
-	rxq->cksum_enabled = dev->data->dev_conf.rxmode.hw_ip_checksum;
-	rxq->queue_id = idx;
-	rxq->port_id = dev->data->port_id;
-	rxq->size = desc;
-	rx_desc_free_thresh = RTE_MIN(rx_desc_free_thresh, (desc/2));
-	priv->ppio_params.inqs_params.tcs_params[MRVL_NETA_DEFAULT_TC].size =
-		desc;
-
-	dev->data->rx_queues[idx] = rxq;
-
-	return 0;
-}
-
-/**
- * DPDK callback to release the receive queue.
- *
- * @param rxq
- *   Generic receive queue pointer.
- */
-static void
-mrvlna_rx_queue_release(void *rxq)
-{
-	unsigned int core_id = rte_lcore_id();
-
-	if (core_id == LCORE_ID_ANY)
-		core_id = 0;
-
-	rte_free(rxq);
-}
-
-/**
- * DPDK callback to configure the transmit queue.
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- * @param idx
- *   Transmit queue index.
- * @param desc
- *   Number of descriptors to configure in the queue.
- * @param socket
- *   NUMA socket on which memory must be allocated.
- * @param conf
- *   Tx queue configuration parameters.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_tx_queue_setup(struct rte_eth_dev *dev, uint16_t idx, uint16_t desc,
-		    unsigned int socket, const struct rte_eth_txconf *conf)
-{
-	struct neta_priv *priv = dev->data->dev_private;
-	struct neta_txq *txq;
-
-	if (dev->data->tx_queues[idx]) {
-		rte_free(dev->data->tx_queues[idx]);
-		dev->data->tx_queues[idx] = NULL;
-	}
-
-	txq = rte_zmalloc_socket("txq", sizeof(*txq), 0, socket);
-	if (!txq)
-		return -ENOMEM;
-
-	txq->priv = priv;
-	txq->queue_id = idx;
-	txq->port_id = dev->data->port_id;
-	txq->tx_deferred_start = conf->tx_deferred_start;
-	dev->data->tx_queues[idx] = txq;
-
-	priv->ppio_params.outqs_params.outqs_params[idx].size = desc;
-	priv->ppio_params.outqs_params.outqs_params[idx].weight = 1;
-
-	return 0;
-}
-
-/**
- * DPDK callback to release the transmit queue.
- *
- * @param txq
- *   Generic transmit queue pointer.
- */
-static void
-mrvlna_tx_queue_release(void *txq)
-{
-	struct neta_txq *q = txq;
-
-	if (!q)
-		return;
-
-	rte_free(q);
-}
-
-/**
- * DPDK callback to start the device.
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- *
- * @return
- *   0 on success, negative errno value on failure.
- */
-static int
-mrvlna_dev_start(struct rte_eth_dev *dev)
-{
-	struct neta_priv *priv = dev->data->dev_private;
-	char match[MRVLNA_MATCH_LEN];
-	int ret = 0, i;
-
-	priv->ppio_id = dev->data->port_id;
-	snprintf(match, sizeof(match), "eth%d", priv->ppio_id);
-	priv->ppio_params.match = match;
-
-	ret = neta_ppio_init(&priv->ppio_params, &priv->ppio);
-	if (ret) {
-		RTE_LOG(ERR, PMD, "Failed to init ppio\n");
-		return ret;
-	}
-	/* Unify port id with MUSDK */
-	priv->ppio_id = priv->ppio->port_id;
-
-	/* Allocate buffers */
-	for (i=0; i < dev->data->nb_rx_queues; i++) {
-		struct neta_rxq *rxq = dev->data->rx_queues[i];
-		ret = mrvlna_buffs_alloc(priv, rxq, rxq->size);
-		if (ret) {
-			rte_free(rxq);
-			return ret;
-		}
-	}
-
-	ret = mrvlna_dev_set_link_up(dev);
-	if (ret) {
-		RTE_LOG(ERR, PMD, "Failed to set link up\n");
-		goto out;
-	}
-
-	/* start tx queues */
-	for (i = 0; i < dev->data->nb_tx_queues; i++) {
-		struct neta_txq *txq = dev->data->tx_queues[i];
-
-		dev->data->tx_queue_state[i] = RTE_ETH_QUEUE_STATE_STARTED;
-
-		if (!txq->tx_deferred_start)
-			continue;
-
-		/* TODO */
-		/*
-		 * All txqs are started by default. Stop them
-		 * so that tx_deferred_start works as expected.
-		 */
-	}
-
-	return 0;
-
-out:
-	RTE_LOG(ERR, PMD, "Failed to start device\n");
-	neta_ppio_deinit(priv->ppio);
-	return ret;
-}
-
-
-
-/**
- * DPDK callback to stop the device.
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- */
-static void
-mrvlna_dev_stop(struct rte_eth_dev *dev)
-{
-	struct neta_priv *priv = dev->data->dev_private;
-	int i;
-
-	mrvlna_dev_set_link_down(dev);
-	RTE_LOG(INFO, PMD, "Flushing rx queues\n");
-	for (i = 0; i < dev->data->nb_rx_queues; i++) {
-		struct neta_rxq *rxq = dev->data->rx_queues[i];
-		struct neta_ppio_desc descs[MRVL_NETA_RXD_MAX];
-
-		mrvlna_rx_queue_flush(rxq, descs);
-	}
-
-	RTE_LOG(INFO, PMD, "Flushing tx queues\n");
-	for (i = 0; i < dev->data->nb_tx_queues; i++) {
-		struct neta_txq *txq = dev->data->tx_queues[i];
-
-		mrvlna_tx_queue_flush(txq);
-	}
-
-	neta_ppio_deinit(priv->ppio);
-
-	priv->ppio = NULL;
-}
-
-/**
- * DPDK callback to retrieve physical link information.
- *
- * @param dev
- *   Pointer to Ethernet device structure.
- * @param wait_to_complete
- *   Wait for request completion (ignored).
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_link_update(struct rte_eth_dev *dev, int wait_to_complete __rte_unused)
-{
-	/*
-	 * TODO
-	 * once MUSDK provides necessary API use it here
-	 */
-	struct neta_priv *priv = dev->data->dev_private;
-	struct ethtool_cmd edata;
-	struct ifreq req;
-	int ret, fd, link_up;
-
-	if (!priv->ppio)
-		return -EPERM;
-
-	edata.cmd = ETHTOOL_GSET;
-
-	strcpy(req.ifr_name, dev->data->name);
-	req.ifr_data = (void *)&edata;
-
-	fd = socket(AF_INET, SOCK_DGRAM, 0);
-	if (fd == -1)
-		return -EFAULT;
-	ret = ioctl(fd, SIOCETHTOOL, &req);
-	if (ret == -1) {
-		close(fd);
-		return -EFAULT;
-	}
-
-	close(fd);
-
-	switch (ethtool_cmd_speed(&edata)) {
-	case SPEED_10:
-		dev->data->dev_link.link_speed = ETH_SPEED_NUM_10M;
-		break;
-	case SPEED_100:
-		dev->data->dev_link.link_speed = ETH_SPEED_NUM_100M;
-		break;
-	case SPEED_1000:
-		dev->data->dev_link.link_speed = ETH_SPEED_NUM_1G;
-		break;
-	case SPEED_2500:
-		dev->data->dev_link.link_speed = ETH_SPEED_NUM_2_5G;
-		break;
-	default:
-		dev->data->dev_link.link_speed = ETH_SPEED_NUM_NONE;
-	}
-
-	dev->data->dev_link.link_duplex = edata.duplex ? ETH_LINK_FULL_DUPLEX :
-							 ETH_LINK_HALF_DUPLEX;
-	dev->data->dev_link.link_autoneg = edata.autoneg ? ETH_LINK_AUTONEG :
-							   ETH_LINK_FIXED;
-
-	neta_ppio_get_link_state(priv->ppio, &link_up);
-	dev->data->dev_link.link_status = link_up ? ETH_LINK_UP : ETH_LINK_DOWN;
-
-	return 0;
-}
-
-static const struct eth_dev_ops mrvlna_ops = {
-	.dev_configure = mrvlna_dev_configure,
-	.dev_start = mrvlna_dev_start,
-	.dev_stop = mrvlna_dev_stop,
-	.dev_set_link_up = mrvlna_dev_set_link_up,
-	.dev_set_link_down = mrvlna_dev_set_link_down,
-	.link_update = mrvlna_link_update,
-	.mtu_set = mrvlna_mtu_set,
-	.dev_infos_get = mrvlna_dev_infos_get,
-	.rx_queue_setup = mrvlna_rx_queue_setup,
-	.rx_queue_release = mrvlna_rx_queue_release,
-	.tx_queue_setup = mrvlna_tx_queue_setup,
-	.tx_queue_release = mrvlna_tx_queue_release,
-};
-
-/**
- * Create private device structure.
- *
- * @param dev_name
- *   Pointer to the port name passed in the initialization parameters.
- *
- * @return
- *   Pointer to the newly allocated private device structure.
- */
-static struct neta_priv *
-mrvlna_priv_create(const char *dev_name)
-{
-	char match[MRVLNA_MATCH_LEN];
-	struct neta_priv *priv;
-
-	priv = rte_zmalloc_socket(dev_name, sizeof(*priv), 0, rte_socket_id());
-	if (!priv)
-		return NULL;
-
-	snprintf(match, sizeof(match), "pool-%d", priv->pp_id);
-
-	return priv;
-}
-
-/**
- * Create device representing Ethernet port.
- *
- * @param name
- *   Pointer to the port's name.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-mrvlna_eth_dev_create(struct rte_vdev_device *vdev, const char *name)
-{
-	int ret, fd = socket(AF_INET, SOCK_DGRAM, 0);
-	struct rte_eth_dev *eth_dev;
-	struct neta_priv *priv;
-	struct ifreq req;
-
-	eth_dev = rte_eth_dev_allocate(name);
-	if (!eth_dev)
-		return -ENOMEM;
-
-	priv = mrvlna_priv_create(name);
-
-	if (!priv) {
-		ret = -ENOMEM;
-		goto out_free_dev;
-	}
-
-	eth_dev->data->mac_addrs =
-		rte_zmalloc("mac_addrs",
-			    ETHER_ADDR_LEN * MRVLNA_MAC_ADDRS_MAX, 0);
-	if (!eth_dev->data->mac_addrs) {
-		RTE_LOG(ERR, PMD, "Failed to allocate space for eth addrs\n");
-		ret = -ENOMEM;
-		goto out_free_priv;
-	}
-
-	memset(&req, 0, sizeof(req));
-	strcpy(req.ifr_name, name);
-	ret = ioctl(fd, SIOCGIFHWADDR, &req);
-	if (ret)
-		goto out_free_mac;
-
-	memcpy(eth_dev->data->mac_addrs[0].addr_bytes,
-	       req.ifr_addr.sa_data, ETHER_ADDR_LEN);
-
-	eth_dev->rx_pkt_burst = mrvlna_rx_pkt_burst;
-	eth_dev->tx_pkt_burst = mrvlna_tx_pkt_burst;
-	eth_dev->data->kdrv = RTE_KDRV_NONE;
-	eth_dev->data->dev_private = priv;
-	eth_dev->device = &vdev->device;
-	eth_dev->dev_ops = &mrvlna_ops;
-
-	return 0;
-out_free_mac:
-	rte_free(eth_dev->data->mac_addrs);
-out_free_dev:
-	rte_eth_dev_release_port(eth_dev);
-out_free_priv:
-	rte_free(priv);
-
-	return ret;
-}
-
-/**
- * Cleanup previously created device representing Ethernet port.
- *
- * @param name
- *   Pointer to the port name.
- */
-static void
-mrvlna_eth_dev_destroy(const char *name)
-{
-	struct rte_eth_dev *eth_dev;
-	struct neta_priv *priv;
-
-	eth_dev = rte_eth_dev_allocated(name);
-	if (!eth_dev)
-		return;
-
-	priv = eth_dev->data->dev_private;
-	rte_free(priv);
-	rte_free(eth_dev->data->mac_addrs);
-	rte_eth_dev_release_port(eth_dev);
-}
-
-/**
- * DPDK callback to register the virtual device.
- *
- * @param vdev
- *   Pointer to the virtual device.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-rte_pmd_mrvlna_probe(struct rte_vdev_device *vdev)
-{
-	struct rte_kvargs *kvlist;
-	struct mrvlna_ifnames ifnames;
-	int ret = -EINVAL;
-	uint32_t i, ifnum;
-	const char *params;
-
-	params = rte_vdev_device_args(vdev);
-	if (!params)
-		return -EINVAL;
-
-	kvlist = rte_kvargs_parse(params, valid_args);
-	if (!kvlist)
-		return -EINVAL;
-
-	ifnum = rte_kvargs_count(kvlist, MRVL_IFACE_NAME_ARG);
-	if (ifnum > RTE_DIM(ifnames.names))
-		goto out_free_kvlist;
-
-	ifnames.idx = 0;
-	rte_kvargs_process(kvlist, MRVL_IFACE_NAME_ARG,
-			   mrvlna_ifnames_get, &ifnames);
-
-	/*
-	 * The below system initialization should be done only once,
-	 * on the first provided configuration file
-	 */
-	if (mrvlna_dev_num)
-		goto init_devices;
-
-	RTE_LOG(INFO, PMD, "Perform MUSDK initializations\n");
-	/*
-	 * ret == -EEXIST is correct, it means DMA
-	 * has been already initialized (by another PMD).
-	 */
-	ret = mv_sys_dma_mem_init(MRVLNA_MUSDK_DMA_MEMSIZE);
-	if (ret < 0) {
-		if (ret != -EEXIST)
-			goto out_free_kvlist;
-		else
-			RTE_LOG(INFO, PMD,
-					"DMA memory has been already initialized by a different driver.\n");
-	}
-
-	ret = mrvlna_neta_init();
-	if (ret) {
-		RTE_LOG(ERR, PMD, "Failed to init NETA!\n");
-		goto out_deinit_dma;
-	}
-
-	mrvlna_lcore_first = RTE_MAX_LCORE;
-	mrvlna_lcore_last = 0;
-
-init_devices:
-	for (i = 0; i < ifnum; i++) {
-		RTE_LOG(INFO, PMD, "Creating %s\n", ifnames.names[i]);
-		ret = mrvlna_eth_dev_create(vdev, ifnames.names[i]);
-		if (ret)
-			goto out_cleanup;
-	}
-	mrvlna_dev_num += ifnum;
-
-	rte_kvargs_free(kvlist);
-
-	return 0;
-out_cleanup:
-	for (; i > 0; i--)
-		mrvlna_eth_dev_destroy(ifnames.names[i]);
-
-	if (mrvlna_dev_num == 0)
-		mrvlna_neta_deinit();
-out_deinit_dma:
-	if (mrvlna_dev_num == 0)
-		mv_sys_dma_mem_destroy();
-out_free_kvlist:
-	rte_kvargs_free(kvlist);
-
-	return ret;
-}
-
-/**
- * DPDK callback to remove virtual device.
- *
- * @param vdev
- *   Pointer to the removed virtual device.
- *
- * @return
- *   0 on success, negative error value otherwise.
- */
-static int
-rte_pmd_mrvlna_remove(struct rte_vdev_device *vdev)
-{
-	int i;
-	const char *name;
-
-	name = rte_vdev_device_name(vdev);
-	if (!name)
-		return -EINVAL;
-
-	RTE_LOG(INFO, PMD, "Removing %s\n", name);
-
-	for (i = 0; i < rte_eth_dev_count(); i++) {
-		char ifname[RTE_ETH_NAME_MAX_LEN];
-
-		rte_eth_dev_get_name_by_port(i, ifname);
-		mrvlna_eth_dev_destroy(ifname);
-		mrvlna_dev_num--;
-	}
-
-	if (mrvlna_dev_num == 0) {
-		RTE_LOG(INFO, PMD, "Perform MUSDK deinit\n");
-		mrvlna_neta_deinit();
-		mv_sys_dma_mem_destroy();
-	}
-
-	return 0;
-}
-
-static struct rte_vdev_driver pmd_mrvlna_drv = {
-	.probe = rte_pmd_mrvlna_probe,
-	.remove = rte_pmd_mrvlna_remove,
-};
-
-RTE_PMD_REGISTER_VDEV(net_mrvlna, pmd_mrvlna_drv);
-RTE_PMD_REGISTER_ALIAS(net_mrvlna, eth_mrvlna);
diff --git a/drivers/net/mrvlna/mrvlna_ethdev.h b/drivers/net/mrvlna/mrvlna_ethdev.h
deleted file mode 100644
index 11c21a4..0000000
--- a/drivers/net/mrvlna/mrvlna_ethdev.h
+++ /dev/null
@@ -1,101 +0,0 @@
-/*-
- *   BSD LICENSE
- *
- *   Copyright(c) 2017 Marvell International Ltd.
- *   Copyright(c) 2017 Semihalf.
- *   All rights reserved.
- *
- *   Redistribution and use in source and binary forms, with or without
- *   modification, are permitted provided that the following conditions
- *   are met:
- *
- *     * Redistributions of source code must retain the above copyright
- *       notice, this list of conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright
- *       notice, this list of conditions and the following disclaimer in
- *       the documentation and/or other materials provided with the
- *       distribution.
- *     * Neither the name of the copyright holder nor the names of its
- *       contributors may be used to endorse or promote products derived
- *       from this software without specific prior written permission.
- *
- *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _MRVL_ETHDEV_H_
-#define _MRVL_ETHDEV_H_
-
-/*
- * container_of is defined by both DPDK and MUSDK,
- * we'll declare only one version.
- *
- * Note that it is not used in this PMD anyway.
- */
-#ifdef container_of
-#undef container_of
-#endif
-
-#include <drivers/mv_neta.h>
-#include <drivers/mv_neta_ppio.h>
-
-/** Packet offset inside RX buffer. */
-#define MRVL_NETA_PKT_OFFS 64
-
-/** Maximum number of rx/tx queues per port */
-#define MRVL_NETA_RXQ_MAX 8
-#define MRVL_NETA_TXQ_MAX 8
-
-/** Minimum/maximum number of descriptors in tx queue TODO is it? */
-#define MRVL_NETA_TXD_MIN 16
-#define MRVL_NETA_TXD_MAX 2048
-
-/** Tx queue descriptors alignment in B */
-#define MRVL_NETA_TXD_ALIGN 32
-
-/** Minimum/maximum number of descriptors in rx queue TODO is it? */
-#define MRVL_NETA_RXD_MIN 16
-#define MRVL_NETA_RXD_MAX 2048
-
-/** Rx queue descriptors alignment in B */
-#define MRVL_NETA_RXD_ALIGN 32
-
-#define MRVL_NETA_DEFAULT_TC 0
-
-/** Maximum number of descriptors in shadow queue. Must be power of 2 */
-#define MRVL_NETA_TX_SHADOWQ_SIZE MRVL_NETA_TXD_MAX
-
-/** Shadow queue size mask (since shadow queue size is power of 2) */
-#define MRVL_NETA_TX_SHADOWQ_MASK (MRVL_NETA_TX_SHADOWQ_SIZE - 1)
-
-/** Minimum number of sent buffers to release from shadow queue to BM */
-#define MRVL_NETA_BUF_RELEASE_BURST_SIZE	16
-
-#define MRVL_NETA_MTU_TO_MRU(mtu) \
-	((mtu) + MV_MH_SIZE + ETHER_HDR_LEN + ETHER_CRC_LEN)
-#define MRVL_NETA_MRU_TO_MTU(mru) \
-	((mru) - MV_MH_SIZE - ETHER_HDR_LEN + ETHER_CRC_LEN)
-
-struct neta_priv {
-	/* Hot fields, used in fast path. */
-	struct neta_ppio	*ppio;    /**< Port handler pointer */
-
-	uint8_t pp_id;
-	uint8_t ppio_id;	/* ppio port id */
-
-	struct neta_ppio_params ppio_params;
-	uint16_t nb_rx_queues;
-
-	uint64_t rate_max;
-};
-
-#endif /* _MRVL_ETHDEV_H_ */
diff --git a/drivers/net/mrvlna/rte_pmd_mrvlna_version.map b/drivers/net/mrvlna/rte_pmd_mrvlna_version.map
deleted file mode 100644
index 58b9427..0000000
--- a/drivers/net/mrvlna/rte_pmd_mrvlna_version.map
+++ /dev/null
@@ -1,3 +0,0 @@
-DPDK_18.02 {
-	local: *;
-};
diff --git a/drivers/net/mvneta/Makefile b/drivers/net/mvneta/Makefile
new file mode 100644
index 0000000..7828208
--- /dev/null
+++ b/drivers/net/mvneta/Makefile
@@ -0,0 +1,69 @@
+#   BSD LICENSE
+#
+#   Copyright(c) 2017 Marvell International Ltd.
+#   Copyright(c) 2017 Semihalf.
+#   All rights reserved.
+#
+#   Redistribution and use in source and binary forms, with or without
+#   modification, are permitted provided that the following conditions
+#   are met:
+#
+#     * Redistributions of source code must retain the above copyright
+#       notice, this list of conditions and the following disclaimer.
+#     * Redistributions in binary form must reproduce the above copyright
+#       notice, this list of conditions and the following disclaimer in
+#       the documentation and/or other materials provided with the
+#       distribution.
+#     * Neither the name of the copyright holder nor the names of its
+#       contributors may be used to endorse or promote products derived
+#       from this software without specific prior written permission.
+#
+#   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+#   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+#   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+#   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+#   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+#   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+#   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+#   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+#   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+#   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+include $(RTE_SDK)/mk/rte.vars.mk
+
+ifneq ($(MAKECMDGOALS),clean)
+ifneq ($(MAKECMDGOALS),config)
+ifeq ($(LIBMUSDK_PATH),)
+$(error "Please define LIBMUSDK_PATH environment variable")
+endif
+endif
+endif
+
+# library name
+LIB = librte_pmd_mvneta.a
+
+# library version
+LIBABIVER := 1
+
+# versioning export map
+EXPORT_MAP := rte_pmd_mvneta_version.map
+
+# external library dependencies
+CFLAGS += -I$(LIBMUSDK_PATH)/include
+CFLAGS += -DMVCONF_TYPES_PUBLIC
+CFLAGS += -DMVCONF_DMA_PHYS_ADDR_T_PUBLIC
+#CFLAGS += -DMVCONF_PP2_BPOOL_COOKIE_SIZE=32 -DMVCONF_PP2_BPOOL_DMA_ADDR_SIZE=64 
+CFLAGS += -DMVCONF_DMA_PHYS_ADDR_T_SIZE=64
+CFLAGS += $(WERROR_FLAGS)
+CFLAGS += -O3
+LDLIBS += -L$(LIBMUSDK_PATH)/lib
+LDLIBS += -lmusdk
+LDLIBS += -lrte_eal -lrte_mbuf -lrte_mempool -lrte_ring
+LDLIBS += -lrte_ethdev -lrte_net -lrte_kvargs -lrte_cfgfile
+LDLIBS += -lrte_bus_vdev
+
+# library source files
+SRCS-$(CONFIG_RTE_LIBRTE_MVNETA_PMD) += mvneta_ethdev.c
+
+include $(RTE_SDK)/mk/rte.lib.mk
diff --git a/drivers/net/mvneta/mvneta_ethdev.c b/drivers/net/mvneta/mvneta_ethdev.c
new file mode 100644
index 0000000..0a2684f
--- /dev/null
+++ b/drivers/net/mvneta/mvneta_ethdev.c
@@ -0,0 +1,1365 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright(c) 2017 Marvell International Ltd.
+ *   Copyright(c) 2017 Semihalf.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Semihalf nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <rte_ethdev_driver.h>
+#include <rte_kvargs.h>
+#include <rte_log.h>
+#include <rte_malloc.h>
+#include <rte_bus_vdev.h>
+
+#include <stdio.h>
+#include <fcntl.h>
+#include <linux/ethtool.h>
+#include <linux/sockios.h>
+#include <net/if.h>
+#include <net/if_arp.h>
+#include <sys/ioctl.h>
+#include <sys/socket.h>
+#include <sys/stat.h>
+#include <sys/types.h>
+
+#include "mvneta_ethdev.h"
+
+
+#define MRVL_IFACE_NAME_ARG "iface"
+#define MRVL_CFG_ARG "cfg"
+
+#define MVNETA_COOKIE_ADDR_INVALID ~0ULL
+
+#define MVNETA_COOKIE_HIGH_ADDR_SHIFT	(sizeof(neta_cookie_t) * 8)
+#define MVNETA_COOKIE_HIGH_ADDR_MASK	(~0ULL << MVNETA_COOKIE_HIGH_ADDR_SHIFT)
+
+#define MVNETA_MUSDK_DMA_MEMSIZE 41943040 /* (40 * 1024 * 1024) */
+
+#define MVNETA_PKT_SIZE_MAX (16382 - MV_MH_SIZE) /* 9700B */
+#define MVNETA_DEFAULT_MTU	1500
+
+#define MVNETA_MAC_ADDRS_MAX 256 /*16 UC, 256 IP, 256 MC/BC */
+/** Maximum length of a match string */
+#define MVNETA_MATCH_LEN 16
+
+#define MVNETA_PKT_EFFEC_OFFS (MRVL_NETA_PKT_OFFS + MV_MH_SIZE)
+
+uint64_t cookie_addr_high = MVNETA_COOKIE_ADDR_INVALID;
+uint16_t rx_desc_free_thresh = MRVL_NETA_BUF_RELEASE_BURST_SIZE;
+
+static const char * const valid_args[] = {
+	MRVL_IFACE_NAME_ARG,
+	NULL
+};
+
+struct mvneta_ifnames {
+	const char *names[NETA_NUM_ETH_PPIO];
+	int idx;
+};
+
+/*
+ * To use buffer harvesting based on loopback port shadow queue structure
+ * was introduced for buffers information bookkeeping.
+ *
+ * Before sending the packet, related buffer information is
+ * stored in shadow queue. After packet is transmitted no longer used
+ * packet buffer is released back to it's original hardware pool,
+ * on condition it originated from interface.
+ * In case it  was generated by application itself i.e: mbuf->port field is
+ * 0xff then its released to software mempool.
+ */
+struct neta_shadow_txq {
+	int head;           /* write index - used when sending buffers */
+	int tail;           /* read index - used when releasing buffers */
+	u16 size;           /* queue occupied size */
+	u16 num_to_release; /* number of buffers sent, that can be released */
+	struct neta_buff_inf ent[MRVL_NETA_TX_SHADOWQ_SIZE]; /* q entries */
+};
+
+
+struct neta_rxq {
+	struct neta_priv *priv;
+	struct rte_mempool *mp;
+	int queue_id;
+	int port_id;
+	int size;
+	int cksum_enabled;
+	uint64_t bytes_recv;
+	uint64_t drop_mac;
+	uint64_t pkts_processed;
+};
+
+
+struct neta_txq {
+	struct neta_priv *priv;
+	int queue_id;
+	int port_id;
+	uint64_t bytes_sent;
+	struct neta_shadow_txq shadow_txqs[RTE_MAX_LCORE];
+	int tx_deferred_start;
+};
+
+static int mvneta_dev_num;
+static int mvneta_lcore_first;
+static int mvneta_lcore_last;
+
+
+
+/**
+ * Allocate buffers from mempool
+ * and store addresses in rx descriptors.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_buffs_alloc(struct neta_priv *priv, struct neta_rxq *rxq, int num)
+{
+	struct rte_mbuf *mbufs[MRVL_NETA_TXD_MAX];
+	struct neta_buff_inf entries[MRVL_NETA_TXD_MAX];
+	uint16_t nb_desc;
+	int ret, i;
+
+	nb_desc = num;
+	ret = rte_pktmbuf_alloc_bulk(rxq->mp, mbufs, nb_desc);
+	if (ret)
+		RTE_LOG(ERR, PMD,
+				"Failed to allocate %u mbufs.\n", nb_desc);
+
+
+	if (cookie_addr_high == MVNETA_COOKIE_ADDR_INVALID)
+		cookie_addr_high =
+			(uint64_t)mbufs[0] & MVNETA_COOKIE_HIGH_ADDR_MASK;
+
+	for (i = 0; i < nb_desc; i++) {
+		if (((uint64_t)mbufs[i] & MVNETA_COOKIE_HIGH_ADDR_MASK)
+			!= cookie_addr_high) {
+			RTE_LOG(ERR, PMD,
+				"mbuf virtual addr high 0x%lx out of range\n",
+				(uint64_t)mbufs[i] >> 32);
+			goto out;
+		}
+	}
+
+	for (i = 0; i < nb_desc; i++) {
+		entries[i].addr = rte_mbuf_data_iova_default(mbufs[i]);
+		entries[i].cookie = (neta_cookie_t)(uint64_t)mbufs[i];
+	}
+	ret = neta_ppio_inq_put_buffs(priv->ppio, rxq->queue_id, entries, &nb_desc);
+	if (ret) {
+		RTE_LOG(ERR, PMD,
+				"Failed to fill rx desc\n");
+		return ret;
+	}
+
+	return 0;
+
+out:
+	for (; i < nb_desc; i++)
+		rte_pktmbuf_free(mbufs[i]);
+
+	return -1;
+}
+
+/**
+ * Return mbufs to mempool.
+ *
+ * @param rxq
+ *    Pointer to rx queue structure
+ * @param desc
+ *    Array of rx descriptors
+ */
+static void
+mvneta_recv_buffs_free(struct neta_ppio_desc *desc, uint16_t num)
+{
+	uint64_t addr;
+	uint8_t i;
+
+	for (i = 0; i < num; i++) {
+		if (desc) {
+			addr = cookie_addr_high |
+					neta_ppio_inq_desc_get_cookie(desc);
+			if (addr)
+				rte_pktmbuf_free((struct rte_mbuf *)addr);
+			desc++;
+		}
+	}
+}
+
+/**
+ * Release already sent buffers to mempool.
+ *
+ * @param ppio
+ *   Pointer to the port structure.
+ * @param sq
+ *   Pointer to the shadow queue.
+ * @param qid
+ *   Queue id number.
+ * @param force
+ *   Force releasing packets.
+ */
+static inline void
+mvneta_sent_buffers_free(struct neta_ppio *ppio,
+		struct neta_shadow_txq *sq, int qid, int force)
+{
+	struct neta_buff_inf *entry;
+	uint16_t nb_done = 0;
+	int i;
+	int tail = sq->tail;
+
+	neta_ppio_get_num_outq_done(ppio, qid, &nb_done);
+
+	sq->num_to_release += nb_done;
+
+	if (likely(!force &&
+		   sq->num_to_release < MRVL_NETA_BUF_RELEASE_BURST_SIZE))
+		return;
+
+	nb_done = sq->num_to_release;
+	sq->num_to_release = 0;
+
+	for (i = 0; i < nb_done; i++) {
+		entry = &sq->ent[tail];
+
+		if (unlikely(!entry->addr)) {
+			RTE_LOG(ERR, PMD,
+				"Shadow memory @%d: cookie(%lx), pa(%lx)!\n",
+				sq->tail, (u64)entry->cookie,
+				(u64)entry->addr);
+			tail = (tail + 1) & MRVL_NETA_TX_SHADOWQ_MASK;;
+			continue;
+		}
+
+		struct rte_mbuf *mbuf;
+
+		mbuf = (struct rte_mbuf *)
+			   (cookie_addr_high | entry->cookie);
+		rte_pktmbuf_free(mbuf);
+		tail = (tail + 1) & MRVL_NETA_TX_SHADOWQ_MASK;
+	}
+
+	sq->tail = tail;
+	sq->size -= nb_done;
+}
+
+/**
+ * Flush single receive queue.
+ *
+ * @param rxq
+ *   Pointer to rx queue structure.
+ * @param descs
+ *   Array of rx descriptors
+ */
+static void
+mvneta_rx_queue_flush(struct neta_rxq *rxq, struct neta_ppio_desc *descs)
+{
+	int ret, num;
+
+	do {
+		num = MRVL_NETA_RXD_MAX;
+		ret = neta_ppio_recv(rxq->priv->ppio,
+					rxq->queue_id,
+					descs, (uint16_t *)&num);
+		mvneta_recv_buffs_free(descs, num);
+		rxq->pkts_processed += num;
+	} while (ret == 0 && num);
+
+}
+
+/**
+ * Flush single transmit queue.
+ *
+ * @param txq
+ *     Pointer to tx queue structure
+ */
+static void
+mvneta_tx_queue_flush(struct neta_txq *txq)
+{
+	int i;
+
+	for (i = 0; i < RTE_MAX_LCORE; i++) {
+		struct neta_shadow_txq *sq =
+				&txq->shadow_txqs[i];
+
+		/* free the rest of them */
+		while (sq->tail != sq->head) {
+			uint64_t addr = cookie_addr_high |
+				sq->ent[sq->tail].cookie;
+			rte_pktmbuf_free(
+				(struct rte_mbuf *)addr);
+			sq->tail = (sq->tail + 1) & MRVL_NETA_TX_SHADOWQ_MASK;
+		}
+		memset(sq, 0, sizeof(*sq));
+	}
+}
+
+/**
+ * Deinitialize packet processor.
+ */
+static void
+mvneta_neta_deinit(void)
+{
+	neta_deinit();
+}
+
+/**
+ * Initialize packet processor.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_neta_init(void)
+{
+	return neta_init();
+}
+
+/**
+ * Callback used by rte_kvargs_process() during argument parsing.
+ *
+ * @param key
+ *   Pointer to the parsed key (unused).
+ * @param value
+ *   Pointer to the parsed value.
+ * @param extra_args
+ *   Pointer to the extra arguments which contains address of the
+ *   table of pointers to parsed interface names.
+ *
+ * @return
+ *   Always 0.
+ */
+static int
+mvneta_ifnames_get(const char *key __rte_unused, const char *value,
+		 void *extra_args)
+{
+	struct mvneta_ifnames *ifnames = extra_args;
+
+	ifnames->names[ifnames->idx++] = value;
+
+	return 0;
+}
+
+/**
+ * Return packet type information and l3/l4 offsets.
+ *
+ * @param desc
+ *   Pointer to the received packet descriptor.
+ * @param l3_offset
+ *   l3 packet offset.
+ * @param l4_offset
+ *   l4 packet offset.
+ *
+ * @return
+ *   Packet type information.
+ */
+static inline uint64_t
+mvneta_desc_to_packet_type_and_offset(struct neta_ppio_desc *desc,
+				    uint8_t *l3_offset, uint8_t *l4_offset)
+{
+	enum neta_inq_l3_type l3_type;
+	enum neta_inq_l4_type l4_type;
+	uint64_t packet_type;
+
+	neta_ppio_inq_desc_get_l3_info(desc, &l3_type, l3_offset);
+	neta_ppio_inq_desc_get_l4_info(desc, &l4_type, l4_offset);
+
+	packet_type = RTE_PTYPE_L2_ETHER;
+
+	switch (l3_type) {
+	case NETA_INQ_L3_TYPE_IPV4_BAD:
+	case NETA_INQ_L3_TYPE_IPV4_OK:
+		packet_type |= RTE_PTYPE_L3_IPV4;
+		break;
+	case NETA_INQ_L3_TYPE_IPV6:
+		packet_type |= RTE_PTYPE_L3_IPV6;
+		break;
+	default:
+		packet_type |= RTE_PTYPE_UNKNOWN;
+		RTE_LOG(DEBUG, PMD, "Failed to recognize l3 packet type\n");
+		break;
+	}
+
+	switch (l4_type) {
+	case NETA_INQ_L4_TYPE_TCP:
+		packet_type |= RTE_PTYPE_L4_TCP;
+		break;
+	case NETA_INQ_L4_TYPE_UDP:
+		packet_type |= RTE_PTYPE_L4_UDP;
+		break;
+	default:
+		packet_type |= RTE_PTYPE_UNKNOWN;
+		RTE_LOG(DEBUG, PMD, "Failed to recognize l4 packet type\n");
+		break;
+	}
+
+	return packet_type;
+}
+
+/**
+ * DPDK callback for transmit.
+ *
+ * @param txq
+ *   Generic pointer transmit queue.
+ * @param tx_pkts
+ *   Packets to transmit.
+ * @param nb_pkts
+ *   Number of packets in array.
+ *
+ * @return
+ *   Number of packets successfully transmitted.
+ */
+static uint16_t
+mvneta_tx_pkt_burst(void *txq, struct rte_mbuf **tx_pkts, uint16_t nb_pkts)
+{
+	struct neta_txq *q = txq;
+	struct neta_shadow_txq *sq;
+	struct neta_ppio_desc descs[nb_pkts];
+	unsigned int core_id = rte_lcore_id();
+
+	int i, bytes_sent = 0;
+	uint16_t num, sq_free_size;
+	uint64_t addr;
+
+	sq = &q->shadow_txqs[core_id];
+	if (unlikely(!q->priv->ppio))
+		return 0;
+
+	if (sq->size)
+		mvneta_sent_buffers_free(q->priv->ppio,
+				sq, q->queue_id, 0);
+
+	sq_free_size = MRVL_NETA_TX_SHADOWQ_SIZE - sq->size - 1;
+	if (unlikely(nb_pkts > sq_free_size)) {
+		RTE_LOG(DEBUG, PMD,
+			"No room in shadow queue for %d packets! %d packets will be sent.\n",
+			nb_pkts, sq_free_size);
+		nb_pkts = sq_free_size;
+	}
+
+
+	for (i = 0; i < nb_pkts; i++) {
+		struct rte_mbuf *mbuf = tx_pkts[i];
+
+		sq->ent[sq->head].cookie = (neta_cookie_t)(uint64_t)mbuf;
+		sq->ent[sq->head].addr = rte_mbuf_data_iova_default(mbuf);
+		sq->head = (sq->head + 1) & MRVL_NETA_TX_SHADOWQ_MASK;
+		sq->size++;
+
+		neta_ppio_outq_desc_reset(&descs[i]);
+		neta_ppio_outq_desc_set_phys_addr(&descs[i],
+						 rte_pktmbuf_iova(mbuf));
+		neta_ppio_outq_desc_set_pkt_offset(&descs[i], 0);
+		neta_ppio_outq_desc_set_pkt_len(&descs[i],
+					       (rte_pktmbuf_pkt_len(mbuf) - (ETH_FCS_LEN + MV_MH_SIZE)));
+
+		bytes_sent += rte_pktmbuf_pkt_len(mbuf);
+
+		/* TODO : prepare & set proto info in the descriptor */
+
+	}
+	num = nb_pkts;
+	neta_ppio_send(q->priv->ppio, q->queue_id, descs, &nb_pkts);
+
+
+	/* number of packets that were not sent */
+	if (unlikely(num > nb_pkts)) {
+		for (i = nb_pkts; i < num; i++) {
+			sq->head = (MRVL_NETA_TX_SHADOWQ_SIZE + sq->head - 1) &
+				MRVL_NETA_TX_SHADOWQ_MASK;
+			addr = cookie_addr_high | sq->ent[sq->head].cookie;
+			bytes_sent -=
+				rte_pktmbuf_pkt_len((struct rte_mbuf *)addr);
+		}
+		sq->size -= num - nb_pkts;
+	}
+
+	q->bytes_sent += bytes_sent;
+
+	return nb_pkts;
+}
+
+/**
+ * DPDK callback for receive.
+ *
+ * @param rxq
+ *   Generic pointer to the receive queue.
+ * @param rx_pkts
+ *   Array to store received packets.
+ * @param nb_pkts
+ *   Maximum number of packets in array.
+ *
+ * @return
+ *   Number of packets successfully received.
+ */
+static uint16_t
+mvneta_rx_pkt_burst(void *rxq, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
+{
+	struct neta_rxq *q = rxq;
+	struct neta_ppio_desc descs[nb_pkts];
+	int i, ret, rx_done = 0;
+
+	if (unlikely(!q->priv->ppio))
+		return 0;
+
+	ret = neta_ppio_recv(q->priv->ppio, q->queue_id,
+			descs, &nb_pkts);
+
+	if (unlikely(ret < 0)) {
+		RTE_LOG(ERR, PMD, "Failed to receive packets\n");
+		return 0;
+	}
+
+	for (i = 0; i < nb_pkts; i++) {
+		struct rte_mbuf *mbuf;
+		uint8_t l3_offset, l4_offset;
+		enum neta_inq_desc_status status;
+		uint64_t addr;
+
+		addr = cookie_addr_high |
+			neta_ppio_inq_desc_get_cookie(&descs[i]);
+		mbuf = (struct rte_mbuf *)addr;
+
+		rte_pktmbuf_reset(mbuf);
+
+		/* drop packet in case of mac, overrun or resource error */
+		status = neta_ppio_inq_desc_get_l2_pkt_error(&descs[i]);
+		if (unlikely(status != NETA_DESC_ERR_OK)) {
+			/* Release the mbuf to the mempool since
+			 * it won't be transferred to tx path */
+			rte_pktmbuf_free(mbuf);
+			q->drop_mac++;
+			continue;
+		}
+
+		mbuf->data_off += MVNETA_PKT_EFFEC_OFFS;
+		mbuf->pkt_len = neta_ppio_inq_desc_get_pkt_len(&descs[i]);
+		mbuf->data_len = mbuf->pkt_len;
+		mbuf->port = q->port_id;
+		mbuf->packet_type =
+			mvneta_desc_to_packet_type_and_offset(&descs[i],
+								&l3_offset,
+								&l4_offset);
+		mbuf->l2_len = l3_offset;
+		mbuf->l3_len = l4_offset - l3_offset;
+
+		/* TODO set here offloads&recv checksum */
+
+		rx_pkts[rx_done++] = mbuf;
+		q->bytes_recv += mbuf->pkt_len;
+	}
+	q->pkts_processed += rx_done;
+
+	if (q->pkts_processed > rx_desc_free_thresh) {
+		ret = mvneta_buffs_alloc(q->priv, q, rx_desc_free_thresh);
+		if (ret)
+			RTE_LOG(ERR, PMD, "Refill failed\n");
+		q->pkts_processed -= rx_desc_free_thresh;
+	}
+
+	return rx_done;
+}
+
+/**
+ * Ethernet device configuration.
+ *
+ * Prepare the driver for a given number of TX and RX queues and
+ * configure RSS if supported.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_dev_configure(struct rte_eth_dev *dev)
+{
+	struct neta_priv *priv = dev->data->dev_private;
+
+	if (dev->data->dev_conf.rxmode.mq_mode != ETH_MQ_RX_NONE) {
+		RTE_LOG(INFO, PMD, "Unsupported RSS and rx multi queue mode %d\n",
+			dev->data->dev_conf.rxmode.mq_mode);
+		return -EINVAL;
+	}
+
+	if (!dev->data->dev_conf.rxmode.hw_strip_crc) {
+		RTE_LOG(INFO, PMD,
+			"L2 CRC stripping is always enabled in hw\n");
+		dev->data->dev_conf.rxmode.hw_strip_crc = 1;
+	}
+
+	if (dev->data->dev_conf.rxmode.hw_vlan_strip) {
+		RTE_LOG(INFO, PMD, "VLAN stripping not supported\n");
+		return -EINVAL;
+	}
+
+	if (dev->data->dev_conf.rxmode.split_hdr_size) {
+		RTE_LOG(INFO, PMD, "Split headers not supported\n");
+		return -EINVAL;
+	}
+
+	if (dev->data->dev_conf.rxmode.enable_scatter) {
+		RTE_LOG(INFO, PMD, "RX Scatter/Gather not supported\n");
+		return -EINVAL;
+	}
+
+	if (dev->data->dev_conf.rxmode.enable_lro) {
+		RTE_LOG(INFO, PMD, "LRO not supported\n");
+		return -EINVAL;
+	}
+
+	if (dev->data->dev_conf.rxmode.jumbo_frame)
+		dev->data->mtu = dev->data->dev_conf.rxmode.max_rx_pkt_len -
+				 ETHER_HDR_LEN - ETHER_CRC_LEN;
+
+	priv->ppio_params.outqs_params.num_outqs = dev->data->nb_tx_queues;
+	priv->nb_rx_queues = dev->data->nb_rx_queues;
+	/* Default: 1 TC, no QoS supported. */
+	priv->ppio_params.inqs_params.num_tcs = 1;
+	priv->ppio_params.inqs_params.tcs_params[0].pkt_offset = MRVL_NETA_PKT_OFFS;
+	/* TODO check if DPDK has already set mtu to default value */
+	priv->ppio_params.inqs_params.mtu = dev->data->mtu ? dev->data->mtu : MVNETA_DEFAULT_MTU;
+
+	return 0;
+}
+
+/**
+ * DPDK callback to get information about the device.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure (unused).
+ * @param info
+ *   Info structure output buffer.
+ */
+static void
+mvneta_dev_infos_get(struct rte_eth_dev *dev __rte_unused,
+		   struct rte_eth_dev_info *info)
+{
+	info->speed_capa = ETH_LINK_SPEED_10M |
+			   ETH_LINK_SPEED_100M |
+			   ETH_LINK_SPEED_1G |
+			   ETH_LINK_SPEED_2_5G;
+
+	info->max_rx_queues = MRVL_NETA_RXQ_MAX;
+	info->max_tx_queues = MRVL_NETA_TXQ_MAX;
+	info->max_mac_addrs = MVNETA_MAC_ADDRS_MAX;
+
+	info->rx_desc_lim.nb_max = MRVL_NETA_RXD_MAX;
+	info->rx_desc_lim.nb_min = MRVL_NETA_RXD_MIN;
+	info->rx_desc_lim.nb_align = MRVL_NETA_RXD_ALIGN;
+
+	info->tx_desc_lim.nb_max = MRVL_NETA_TXD_MAX;
+	info->tx_desc_lim.nb_min = MRVL_NETA_TXD_MIN;
+	info->tx_desc_lim.nb_align = MRVL_NETA_TXD_ALIGN;
+
+	info->rx_offload_capa = DEV_RX_OFFLOAD_JUMBO_FRAME |
+				DEV_RX_OFFLOAD_IPV4_CKSUM |
+				DEV_RX_OFFLOAD_UDP_CKSUM |
+				DEV_RX_OFFLOAD_TCP_CKSUM;
+
+	info->tx_offload_capa = DEV_TX_OFFLOAD_IPV4_CKSUM |
+				DEV_TX_OFFLOAD_UDP_CKSUM |
+				DEV_TX_OFFLOAD_TCP_CKSUM;
+
+	/* By default packets are dropped if no descriptors are available */
+	info->default_rxconf.rx_drop_en = 1;
+
+	info->max_rx_pktlen = MVNETA_PKT_SIZE_MAX;
+}
+
+/**
+ * DPDK callback to change the MTU.
+ *
+ * Setting the MTU affects hardware MRU (packets larger than the MRU
+ * will be dropped).
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ * @param mtu
+ *   New MTU.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_mtu_set(struct rte_eth_dev *dev, uint16_t mtu)
+{
+	struct neta_priv *priv = dev->data->dev_private;
+	uint16_t mru = MRVL_NETA_MTU_TO_MRU(mtu);
+
+	if (mtu < ETHER_MIN_MTU || mru > MVNETA_PKT_SIZE_MAX) {
+		RTE_LOG(ERR, PMD, "Invalid MTU [%u] or MRU [%u]\n", mtu, mru);
+		return -EINVAL;
+	}
+
+	if (!priv->ppio)
+		return -EPERM;
+
+	/* TODO note this has no effect as mtu set only during initialization */
+	priv->ppio_params.inqs_params.mtu = mtu;
+
+	/* TODO below functions cause hardware undefined behaviour, skipped for now */
+	/*ret = neta_ppio_set_mru(priv->ppio, mru);
+	if (ret)
+		return ret;
+
+	return neta_ppio_set_mtu(priv->ppio, mtu);*/
+	return 0;
+}
+
+/**
+ * DPDK callback to bring the link up.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_dev_set_link_up(struct rte_eth_dev *dev)
+{
+	struct neta_priv *priv = dev->data->dev_private;
+	int ret;
+
+	if (!priv->ppio)
+		return -EPERM;
+
+	ret = neta_ppio_enable(priv->ppio);
+	if (ret)
+		return ret;
+
+	ret = mvneta_mtu_set(dev, dev->data->mtu);
+	if (ret) {
+		neta_ppio_disable(priv->ppio);
+		return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * DPDK callback to bring the link down.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_dev_set_link_down(struct rte_eth_dev *dev)
+{
+	struct neta_priv *priv = dev->data->dev_private;
+
+	if (!priv->ppio)
+		return -EPERM;
+
+	return neta_ppio_disable(priv->ppio);
+}
+
+/**
+ * DPDK callback to configure the receive queue.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ * @param idx
+ *   RX queue index.
+ * @param desc
+ *   Number of descriptors to configure in queue.
+ * @param socket
+ *   NUMA socket on which memory must be allocated.
+ * @param conf
+ *   Thresholds parameters (unused_).
+ * @param mp
+ *   Memory pool for buffer allocations.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_rx_queue_setup(struct rte_eth_dev *dev, uint16_t idx, uint16_t desc,
+		    unsigned int socket,
+		    const struct rte_eth_rxconf *conf __rte_unused,
+		    struct rte_mempool *mp)
+{
+	struct neta_priv *priv = dev->data->dev_private;
+	struct neta_rxq *rxq;
+	uint32_t min_size,
+		 max_rx_pkt_len = dev->data->dev_conf.rxmode.max_rx_pkt_len;
+
+	min_size = rte_pktmbuf_data_room_size(mp) - RTE_PKTMBUF_HEADROOM -
+		   MVNETA_PKT_EFFEC_OFFS;
+	if (min_size < max_rx_pkt_len) {
+		RTE_LOG(ERR, PMD,
+			"Mbuf size must be increased to %u bytes to hold up to %u bytes of data.\n",
+			max_rx_pkt_len + RTE_PKTMBUF_HEADROOM +
+			MVNETA_PKT_EFFEC_OFFS,
+			max_rx_pkt_len);
+		return -EINVAL;
+	}
+
+	if (dev->data->rx_queues[idx]) {
+		rte_free(dev->data->rx_queues[idx]);
+		dev->data->rx_queues[idx] = NULL;
+	}
+
+	rxq = rte_zmalloc_socket("rxq", sizeof(*rxq), 0, socket);
+	if (!rxq)
+		return -ENOMEM;
+
+	rxq->priv = priv;
+	rxq->mp = mp;
+	rxq->cksum_enabled = dev->data->dev_conf.rxmode.hw_ip_checksum;
+	rxq->queue_id = idx;
+	rxq->port_id = dev->data->port_id;
+	rxq->size = desc;
+	rx_desc_free_thresh = RTE_MIN(rx_desc_free_thresh, (desc/2));
+	priv->ppio_params.inqs_params.tcs_params[MRVL_NETA_DEFAULT_TC].size =
+		desc;
+
+	dev->data->rx_queues[idx] = rxq;
+
+	return 0;
+}
+
+/**
+ * DPDK callback to release the receive queue.
+ *
+ * @param rxq
+ *   Generic receive queue pointer.
+ */
+static void
+mvneta_rx_queue_release(void *rxq)
+{
+	unsigned int core_id = rte_lcore_id();
+
+	if (core_id == LCORE_ID_ANY)
+		core_id = 0;
+
+	rte_free(rxq);
+}
+
+/**
+ * DPDK callback to configure the transmit queue.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ * @param idx
+ *   Transmit queue index.
+ * @param desc
+ *   Number of descriptors to configure in the queue.
+ * @param socket
+ *   NUMA socket on which memory must be allocated.
+ * @param conf
+ *   Tx queue configuration parameters.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_tx_queue_setup(struct rte_eth_dev *dev, uint16_t idx, uint16_t desc,
+		    unsigned int socket, const struct rte_eth_txconf *conf)
+{
+	struct neta_priv *priv = dev->data->dev_private;
+	struct neta_txq *txq;
+
+	if (dev->data->tx_queues[idx]) {
+		rte_free(dev->data->tx_queues[idx]);
+		dev->data->tx_queues[idx] = NULL;
+	}
+
+	txq = rte_zmalloc_socket("txq", sizeof(*txq), 0, socket);
+	if (!txq)
+		return -ENOMEM;
+
+	txq->priv = priv;
+	txq->queue_id = idx;
+	txq->port_id = dev->data->port_id;
+	txq->tx_deferred_start = conf->tx_deferred_start;
+	dev->data->tx_queues[idx] = txq;
+
+	priv->ppio_params.outqs_params.outqs_params[idx].size = desc;
+	priv->ppio_params.outqs_params.outqs_params[idx].weight = 1;
+
+	return 0;
+}
+
+/**
+ * DPDK callback to release the transmit queue.
+ *
+ * @param txq
+ *   Generic transmit queue pointer.
+ */
+static void
+mvneta_tx_queue_release(void *txq)
+{
+	struct neta_txq *q = txq;
+
+	if (!q)
+		return;
+
+	rte_free(q);
+}
+
+/**
+ * DPDK callback to start the device.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ *
+ * @return
+ *   0 on success, negative errno value on failure.
+ */
+static int
+mvneta_dev_start(struct rte_eth_dev *dev)
+{
+	struct neta_priv *priv = dev->data->dev_private;
+	char match[MVNETA_MATCH_LEN];
+	int ret = 0, i;
+
+	priv->ppio_id = dev->data->port_id;
+	snprintf(match, sizeof(match), "eth%d", priv->ppio_id);
+	priv->ppio_params.match = match;
+
+	ret = neta_ppio_init(&priv->ppio_params, &priv->ppio);
+	if (ret) {
+		RTE_LOG(ERR, PMD, "Failed to init ppio\n");
+		return ret;
+	}
+	/* Unify port id with MUSDK */
+	priv->ppio_id = priv->ppio->port_id;
+
+	/* Allocate buffers */
+	for (i=0; i < dev->data->nb_rx_queues; i++) {
+		struct neta_rxq *rxq = dev->data->rx_queues[i];
+		ret = mvneta_buffs_alloc(priv, rxq, rxq->size);
+		if (ret) {
+			rte_free(rxq);
+			return ret;
+		}
+	}
+
+	ret = mvneta_dev_set_link_up(dev);
+	if (ret) {
+		RTE_LOG(ERR, PMD, "Failed to set link up\n");
+		goto out;
+	}
+
+	/* start tx queues */
+	for (i = 0; i < dev->data->nb_tx_queues; i++) {
+		struct neta_txq *txq = dev->data->tx_queues[i];
+
+		dev->data->tx_queue_state[i] = RTE_ETH_QUEUE_STATE_STARTED;
+
+		if (!txq->tx_deferred_start)
+			continue;
+
+		/* TODO */
+		/*
+		 * All txqs are started by default. Stop them
+		 * so that tx_deferred_start works as expected.
+		 */
+	}
+
+	return 0;
+
+out:
+	RTE_LOG(ERR, PMD, "Failed to start device\n");
+	neta_ppio_deinit(priv->ppio);
+	return ret;
+}
+
+
+
+/**
+ * DPDK callback to stop the device.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ */
+static void
+mvneta_dev_stop(struct rte_eth_dev *dev)
+{
+	struct neta_priv *priv = dev->data->dev_private;
+	int i;
+
+	mvneta_dev_set_link_down(dev);
+	RTE_LOG(INFO, PMD, "Flushing rx queues\n");
+	for (i = 0; i < dev->data->nb_rx_queues; i++) {
+		struct neta_rxq *rxq = dev->data->rx_queues[i];
+		struct neta_ppio_desc descs[MRVL_NETA_RXD_MAX];
+
+		mvneta_rx_queue_flush(rxq, descs);
+	}
+
+	RTE_LOG(INFO, PMD, "Flushing tx queues\n");
+	for (i = 0; i < dev->data->nb_tx_queues; i++) {
+		struct neta_txq *txq = dev->data->tx_queues[i];
+
+		mvneta_tx_queue_flush(txq);
+	}
+
+	neta_ppio_deinit(priv->ppio);
+
+	priv->ppio = NULL;
+}
+
+/**
+ * DPDK callback to retrieve physical link information.
+ *
+ * @param dev
+ *   Pointer to Ethernet device structure.
+ * @param wait_to_complete
+ *   Wait for request completion (ignored).
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_link_update(struct rte_eth_dev *dev, int wait_to_complete __rte_unused)
+{
+	/*
+	 * TODO
+	 * once MUSDK provides necessary API use it here
+	 */
+	struct neta_priv *priv = dev->data->dev_private;
+	struct ethtool_cmd edata;
+	struct ifreq req;
+	int ret, fd, link_up;
+
+	if (!priv->ppio)
+		return -EPERM;
+
+	edata.cmd = ETHTOOL_GSET;
+
+	strcpy(req.ifr_name, dev->data->name);
+	req.ifr_data = (void *)&edata;
+
+	fd = socket(AF_INET, SOCK_DGRAM, 0);
+	if (fd == -1)
+		return -EFAULT;
+	ret = ioctl(fd, SIOCETHTOOL, &req);
+	if (ret == -1) {
+		close(fd);
+		return -EFAULT;
+	}
+
+	close(fd);
+
+	switch (ethtool_cmd_speed(&edata)) {
+	case SPEED_10:
+		dev->data->dev_link.link_speed = ETH_SPEED_NUM_10M;
+		break;
+	case SPEED_100:
+		dev->data->dev_link.link_speed = ETH_SPEED_NUM_100M;
+		break;
+	case SPEED_1000:
+		dev->data->dev_link.link_speed = ETH_SPEED_NUM_1G;
+		break;
+	case SPEED_2500:
+		dev->data->dev_link.link_speed = ETH_SPEED_NUM_2_5G;
+		break;
+	default:
+		dev->data->dev_link.link_speed = ETH_SPEED_NUM_NONE;
+	}
+
+	dev->data->dev_link.link_duplex = edata.duplex ? ETH_LINK_FULL_DUPLEX :
+							 ETH_LINK_HALF_DUPLEX;
+	dev->data->dev_link.link_autoneg = edata.autoneg ? ETH_LINK_AUTONEG :
+							   ETH_LINK_FIXED;
+
+	neta_ppio_get_link_state(priv->ppio, &link_up);
+	dev->data->dev_link.link_status = link_up ? ETH_LINK_UP : ETH_LINK_DOWN;
+
+	return 0;
+}
+
+static const struct eth_dev_ops mvneta_ops = {
+	.dev_configure = mvneta_dev_configure,
+	.dev_start = mvneta_dev_start,
+	.dev_stop = mvneta_dev_stop,
+	.dev_set_link_up = mvneta_dev_set_link_up,
+	.dev_set_link_down = mvneta_dev_set_link_down,
+	.link_update = mvneta_link_update,
+	.mtu_set = mvneta_mtu_set,
+	.dev_infos_get = mvneta_dev_infos_get,
+	.rx_queue_setup = mvneta_rx_queue_setup,
+	.rx_queue_release = mvneta_rx_queue_release,
+	.tx_queue_setup = mvneta_tx_queue_setup,
+	.tx_queue_release = mvneta_tx_queue_release,
+};
+
+/**
+ * Create private device structure.
+ *
+ * @param dev_name
+ *   Pointer to the port name passed in the initialization parameters.
+ *
+ * @return
+ *   Pointer to the newly allocated private device structure.
+ */
+static struct neta_priv *
+mvneta_priv_create(const char *dev_name)
+{
+	char match[MVNETA_MATCH_LEN];
+	struct neta_priv *priv;
+
+	priv = rte_zmalloc_socket(dev_name, sizeof(*priv), 0, rte_socket_id());
+	if (!priv)
+		return NULL;
+
+	snprintf(match, sizeof(match), "pool-%d", priv->pp_id);
+
+	return priv;
+}
+
+/**
+ * Create device representing Ethernet port.
+ *
+ * @param name
+ *   Pointer to the port's name.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+mvneta_eth_dev_create(struct rte_vdev_device *vdev, const char *name)
+{
+	int ret, fd = socket(AF_INET, SOCK_DGRAM, 0);
+	struct rte_eth_dev *eth_dev;
+	struct neta_priv *priv;
+	struct ifreq req;
+
+	eth_dev = rte_eth_dev_allocate(name);
+	if (!eth_dev)
+		return -ENOMEM;
+
+	priv = mvneta_priv_create(name);
+
+	if (!priv) {
+		ret = -ENOMEM;
+		goto out_free_dev;
+	}
+
+	eth_dev->data->mac_addrs =
+		rte_zmalloc("mac_addrs",
+			    ETHER_ADDR_LEN * MVNETA_MAC_ADDRS_MAX, 0);
+	if (!eth_dev->data->mac_addrs) {
+		RTE_LOG(ERR, PMD, "Failed to allocate space for eth addrs\n");
+		ret = -ENOMEM;
+		goto out_free_priv;
+	}
+
+	memset(&req, 0, sizeof(req));
+	strcpy(req.ifr_name, name);
+	ret = ioctl(fd, SIOCGIFHWADDR, &req);
+	if (ret)
+		goto out_free_mac;
+
+	memcpy(eth_dev->data->mac_addrs[0].addr_bytes,
+	       req.ifr_addr.sa_data, ETHER_ADDR_LEN);
+
+	eth_dev->rx_pkt_burst = mvneta_rx_pkt_burst;
+	eth_dev->tx_pkt_burst = mvneta_tx_pkt_burst;
+	eth_dev->data->kdrv = RTE_KDRV_NONE;
+	eth_dev->data->dev_private = priv;
+	eth_dev->device = &vdev->device;
+	eth_dev->dev_ops = &mvneta_ops;
+
+	return 0;
+out_free_mac:
+	rte_free(eth_dev->data->mac_addrs);
+out_free_dev:
+	rte_eth_dev_release_port(eth_dev);
+out_free_priv:
+	rte_free(priv);
+
+	return ret;
+}
+
+/**
+ * Cleanup previously created device representing Ethernet port.
+ *
+ * @param name
+ *   Pointer to the port name.
+ */
+static void
+mvneta_eth_dev_destroy(const char *name)
+{
+	struct rte_eth_dev *eth_dev;
+	struct neta_priv *priv;
+
+	eth_dev = rte_eth_dev_allocated(name);
+	if (!eth_dev)
+		return;
+
+	priv = eth_dev->data->dev_private;
+	rte_free(priv);
+	rte_free(eth_dev->data->mac_addrs);
+	rte_eth_dev_release_port(eth_dev);
+}
+
+/**
+ * DPDK callback to register the virtual device.
+ *
+ * @param vdev
+ *   Pointer to the virtual device.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+rte_pmd_mvneta_probe(struct rte_vdev_device *vdev)
+{
+	struct rte_kvargs *kvlist;
+	struct mvneta_ifnames ifnames;
+	int ret = -EINVAL;
+	uint32_t i, ifnum;
+	const char *params;
+
+	params = rte_vdev_device_args(vdev);
+	if (!params)
+		return -EINVAL;
+
+	kvlist = rte_kvargs_parse(params, valid_args);
+	if (!kvlist)
+		return -EINVAL;
+
+	ifnum = rte_kvargs_count(kvlist, MRVL_IFACE_NAME_ARG);
+	if (ifnum > RTE_DIM(ifnames.names))
+		goto out_free_kvlist;
+
+	ifnames.idx = 0;
+	rte_kvargs_process(kvlist, MRVL_IFACE_NAME_ARG,
+			   mvneta_ifnames_get, &ifnames);
+
+	/*
+	 * The below system initialization should be done only once,
+	 * on the first provided configuration file
+	 */
+	if (mvneta_dev_num)
+		goto init_devices;
+
+	RTE_LOG(INFO, PMD, "Perform MUSDK initializations\n");
+	/*
+	 * ret == -EEXIST is correct, it means DMA
+	 * has been already initialized (by another PMD).
+	 */
+	ret = mv_sys_dma_mem_init(MVNETA_MUSDK_DMA_MEMSIZE);
+	if (ret < 0) {
+		if (ret != -EEXIST)
+			goto out_free_kvlist;
+		else
+			RTE_LOG(INFO, PMD,
+					"DMA memory has been already initialized by a different driver.\n");
+	}
+
+	ret = mvneta_neta_init();
+	if (ret) {
+		RTE_LOG(ERR, PMD, "Failed to init NETA!\n");
+		goto out_deinit_dma;
+	}
+
+	mvneta_lcore_first = RTE_MAX_LCORE;
+	mvneta_lcore_last = 0;
+
+init_devices:
+	for (i = 0; i < ifnum; i++) {
+		RTE_LOG(INFO, PMD, "Creating %s\n", ifnames.names[i]);
+		ret = mvneta_eth_dev_create(vdev, ifnames.names[i]);
+		if (ret)
+			goto out_cleanup;
+	}
+	mvneta_dev_num += ifnum;
+
+	rte_kvargs_free(kvlist);
+
+	return 0;
+out_cleanup:
+	for (; i > 0; i--)
+		mvneta_eth_dev_destroy(ifnames.names[i]);
+
+	if (mvneta_dev_num == 0)
+		mvneta_neta_deinit();
+out_deinit_dma:
+	if (mvneta_dev_num == 0)
+		mv_sys_dma_mem_destroy();
+out_free_kvlist:
+	rte_kvargs_free(kvlist);
+
+	return ret;
+}
+
+/**
+ * DPDK callback to remove virtual device.
+ *
+ * @param vdev
+ *   Pointer to the removed virtual device.
+ *
+ * @return
+ *   0 on success, negative error value otherwise.
+ */
+static int
+rte_pmd_mvneta_remove(struct rte_vdev_device *vdev)
+{
+	int i;
+	const char *name;
+
+	name = rte_vdev_device_name(vdev);
+	if (!name)
+		return -EINVAL;
+
+	RTE_LOG(INFO, PMD, "Removing %s\n", name);
+
+	for (i = 0; i < rte_eth_dev_count(); i++) {
+		char ifname[RTE_ETH_NAME_MAX_LEN];
+
+		rte_eth_dev_get_name_by_port(i, ifname);
+		mvneta_eth_dev_destroy(ifname);
+		mvneta_dev_num--;
+	}
+
+	if (mvneta_dev_num == 0) {
+		RTE_LOG(INFO, PMD, "Perform MUSDK deinit\n");
+		mvneta_neta_deinit();
+		mv_sys_dma_mem_destroy();
+	}
+
+	return 0;
+}
+
+static struct rte_vdev_driver pmd_mvneta_drv = {
+	.probe = rte_pmd_mvneta_probe,
+	.remove = rte_pmd_mvneta_remove,
+};
+
+RTE_PMD_REGISTER_VDEV(net_mvneta, pmd_mvneta_drv);
+RTE_PMD_REGISTER_ALIAS(net_mvneta, eth_mvneta);
diff --git a/drivers/net/mvneta/mvneta_ethdev.h b/drivers/net/mvneta/mvneta_ethdev.h
new file mode 100644
index 0000000..58bebeb
--- /dev/null
+++ b/drivers/net/mvneta/mvneta_ethdev.h
@@ -0,0 +1,101 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright(c) 2017 Marvell International Ltd.
+ *   Copyright(c) 2017 Semihalf.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of the copyright holder nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _MVNETA_ETHDEV_H_
+#define _MVNETA_ETHDEV_H_
+
+/*
+ * container_of is defined by both DPDK and MUSDK,
+ * we'll declare only one version.
+ *
+ * Note that it is not used in this PMD anyway.
+ */
+#ifdef container_of
+#undef container_of
+#endif
+
+#include <drivers/mv_neta.h>
+#include <drivers/mv_neta_ppio.h>
+
+/** Packet offset inside RX buffer. */
+#define MRVL_NETA_PKT_OFFS 64
+
+/** Maximum number of rx/tx queues per port */
+#define MRVL_NETA_RXQ_MAX 8
+#define MRVL_NETA_TXQ_MAX 8
+
+/** Minimum/maximum number of descriptors in tx queue TODO is it? */
+#define MRVL_NETA_TXD_MIN 16
+#define MRVL_NETA_TXD_MAX 2048
+
+/** Tx queue descriptors alignment in B */
+#define MRVL_NETA_TXD_ALIGN 32
+
+/** Minimum/maximum number of descriptors in rx queue TODO is it? */
+#define MRVL_NETA_RXD_MIN 16
+#define MRVL_NETA_RXD_MAX 2048
+
+/** Rx queue descriptors alignment in B */
+#define MRVL_NETA_RXD_ALIGN 32
+
+#define MRVL_NETA_DEFAULT_TC 0
+
+/** Maximum number of descriptors in shadow queue. Must be power of 2 */
+#define MRVL_NETA_TX_SHADOWQ_SIZE MRVL_NETA_TXD_MAX
+
+/** Shadow queue size mask (since shadow queue size is power of 2) */
+#define MRVL_NETA_TX_SHADOWQ_MASK (MRVL_NETA_TX_SHADOWQ_SIZE - 1)
+
+/** Minimum number of sent buffers to release from shadow queue to BM */
+#define MRVL_NETA_BUF_RELEASE_BURST_SIZE	16
+
+#define MRVL_NETA_MTU_TO_MRU(mtu) \
+	((mtu) + MV_MH_SIZE + ETHER_HDR_LEN + ETHER_CRC_LEN)
+#define MRVL_NETA_MRU_TO_MTU(mru) \
+	((mru) - MV_MH_SIZE - ETHER_HDR_LEN + ETHER_CRC_LEN)
+
+struct neta_priv {
+	/* Hot fields, used in fast path. */
+	struct neta_ppio	*ppio;    /**< Port handler pointer */
+
+	uint8_t pp_id;
+	uint8_t ppio_id;	/* ppio port id */
+
+	struct neta_ppio_params ppio_params;
+	uint16_t nb_rx_queues;
+
+	uint64_t rate_max;
+};
+
+#endif /* _MVNETA_ETHDEV_H_ */
diff --git a/drivers/net/mvneta/rte_pmd_mvneta_version.map b/drivers/net/mvneta/rte_pmd_mvneta_version.map
new file mode 100644
index 0000000..58b9427
--- /dev/null
+++ b/drivers/net/mvneta/rte_pmd_mvneta_version.map
@@ -0,0 +1,3 @@
+DPDK_18.02 {
+	local: *;
+};
diff --git a/mk/rte.app.mk b/mk/rte.app.mk
index a9e7b5b..ce4a2bb 100644
--- a/mk/rte.app.mk
+++ b/mk/rte.app.mk
@@ -155,7 +155,7 @@ else
 _LDLIBS-$(CONFIG_RTE_LIBRTE_MLX5_PMD)       += -lrte_pmd_mlx5 -libverbs -lmlx5
 endif
 _LDLIBS-$(CONFIG_RTE_LIBRTE_MRVL_PMD)       += -lrte_pmd_mrvl -L$(LIBMUSDK_PATH)/lib -lmusdk
-_LDLIBS-$(CONFIG_RTE_LIBRTE_MRVLNA_PMD)     += -lrte_pmd_mrvlna -L$(LIBMUSDK_PATH)/lib -lmusdk
+_LDLIBS-$(CONFIG_RTE_LIBRTE_MVNETA_PMD)     += -lrte_pmd_mvneta -L$(LIBMUSDK_PATH)/lib -lmusdk
 _LDLIBS-$(CONFIG_RTE_LIBRTE_NFP_PMD)        += -lrte_pmd_nfp
 _LDLIBS-$(CONFIG_RTE_LIBRTE_PMD_NULL)       += -lrte_pmd_null
 _LDLIBS-$(CONFIG_RTE_LIBRTE_PMD_PCAP)       += -lrte_pmd_pcap -lpcap
-- 
2.7.4

